{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63c2b1a",
   "metadata": {},
   "source": [
    "# ğŸ”¤ Consigna 5: Modelos NLP + Machine Learning\n",
    "## Trabajo Final - Inteligencia de Negocios 2025\n",
    "\n",
    "**MaestrÃ­a en EconomÃ­a Aplicada - UBA**  \n",
    "**Dataset:** train_bi_2025.csv con descripciones inmobiliarias\n",
    "\n",
    "### ğŸ¯ Consigna 5: Modelos de Aprendizaje + Procesamiento de Lenguaje Natural\n",
    "\n",
    "**5a)** ğŸ“ **RepresentaciÃ³n vectorial** de descripciones: TF-IDF  \n",
    "**5b)** ğŸ”— **Incorporar** representaciÃ³n al dataset original  \n",
    "**5c)** ğŸ¤– **Repetir paso 4** con dataset completo (hÃ­brido):\n",
    "- **Random Forest** + bÃºsqueda hiperparÃ¡metros\n",
    "- **Boosting (XGBoost)** + bÃºsqueda hiperparÃ¡metros  \n",
    "- **Redes Neuronales** + 3 arquitecturas diferentes\n",
    "\n",
    "### ğŸ“Š Baseline a Superar\n",
    "- RMSE: $42,960 (LASSO tradicional)\n",
    "- RÂ²: 0.7284\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41150247",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ConfiguraciÃ³n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cda3f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  TensorFlow disponible\n",
      "âœ… Setup completado - Consigna 5\n",
      "ğŸ“Š RepresentaciÃ³n vectorial: TF-IDF\n",
      "ğŸ”— Modelos hÃ­bridos: Texto + Features tradicionales\n",
      "âœ… Setup completado - Consigna 5\n",
      "ğŸ“Š RepresentaciÃ³n vectorial: TF-IDF\n",
      "ğŸ”— Modelos hÃ­bridos: Texto + Features tradicionales\n"
     ]
    }
   ],
   "source": [
    "# Importaciones esenciales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Neural Networks libraries\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(\"ğŸ§  TensorFlow disponible\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"âš ï¸ TensorFlow no disponible - modelos NN limitados\")\n",
    "\n",
    "# NLP libraries  \n",
    "import nltk\n",
    "import re\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    spanish_stopwords = set(stopwords.words('spanish'))\n",
    "except:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    from nltk.corpus import stopwords\n",
    "    spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "# Plot config\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Setup completado - Consigna 5\")\n",
    "print(\"ğŸ“Š RepresentaciÃ³n vectorial: TF-IDF\")\n",
    "print(\"ğŸ”— Modelos hÃ­bridos: Texto + Features tradicionales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f83be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Sistema YAML centralizado RESTAURADO\n",
      "ğŸ“Š Listo para anÃ¡lisis NLP con datasets grandes\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Sistema de guardado YAML centralizado RESTAURADO\n",
    "import yaml\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "def save_result_to_yaml(resultado, yaml_path='resultados_modelos_nlp.yaml'):\n",
    "    \"\"\"Guarda resultados NLP de forma incremental evitando duplicados\"\"\"\n",
    "    \n",
    "    def convert_tuples_to_lists(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_tuples_to_lists(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, tuple):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_tuples_to_lists(i) for i in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Leer archivo existente\n",
    "    try:\n",
    "        with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "            data = yaml.safe_load(f) or []\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "    \n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    # Convertir y evitar duplicados\n",
    "    resultado_clean = convert_tuples_to_lists(copy.deepcopy(resultado))\n",
    "    \n",
    "    if 'modelo' in resultado_clean:\n",
    "        modelo_nombre = resultado_clean['modelo']\n",
    "        data = [d for d in data if d.get('modelo') != modelo_nombre]\n",
    "        print(f\"ğŸ’¾ Guardando: {modelo_nombre}\")\n",
    "    \n",
    "    data.append(resultado_clean)\n",
    "    \n",
    "    # Guardar\n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(data, f, allow_unicode=True, sort_keys=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Guardado en {yaml_path}\")\n",
    "    return data\n",
    "\n",
    "# ConfiguraciÃ³n para datasets grandes\n",
    "def clean_memory():\n",
    "    \"\"\"Libera memoria cuando sea necesario\"\"\"\n",
    "    gc.collect()\n",
    "    return \"ğŸ§¹ Memoria liberada\"\n",
    "\n",
    "print(\"ğŸ”§ Sistema YAML centralizado RESTAURADO\")\n",
    "print(\"ğŸ“Š Listo para anÃ¡lisis NLP con datasets grandes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89d5e",
   "metadata": {},
   "source": [
    "## ğŸ” ConfiguraciÃ³n de Directorio y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1b62f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ CONFIGURACIÃ“N INICIAL\n",
      "==================================================\n",
      "ğŸ“ Directorio 'models' existe\n",
      "\n",
      "ğŸ“‹ CARGANDO DATASET PARA ANÃLISIS NLP\n",
      "âœ… Dataset filtrado cargado (del notebook 01)\n",
      "ğŸ“Š Dataset: 311,660 Ã— 15 | 377.8 MB\n",
      "ğŸ“ Descripciones vÃ¡lidas: 311,660 (100.0%)\n",
      "ğŸ“ Longitud promedio: 935 caracteres\n",
      "\n",
      "ğŸ’­ Ejemplo - Precio: $190,000\n",
      "   'Apto crÃ©dito PH en PB de 4 ambientes al frente Superficie total de 73mÂ² cubierta de 57mÂ² y descubierta de 16mÂ² Tres dorm...'\n",
      "\n",
      "ğŸ’° Precios: Î¼=$160,807 | mediana=$139,100\n",
      "   Rango: $2,170 - $488,274\n",
      "\n",
      "ğŸ¯ BASELINE A SUPERAR (Consigna 4):\n",
      "   RMSE: $42,960 | RÂ²: 0.7284\n",
      "   Meta: Mejorar con features de texto (NLP)\n",
      "âœ… Dataset filtrado cargado (del notebook 01)\n",
      "ğŸ“Š Dataset: 311,660 Ã— 15 | 377.8 MB\n",
      "ğŸ“ Descripciones vÃ¡lidas: 311,660 (100.0%)\n",
      "ğŸ“ Longitud promedio: 935 caracteres\n",
      "\n",
      "ğŸ’­ Ejemplo - Precio: $190,000\n",
      "   'Apto crÃ©dito PH en PB de 4 ambientes al frente Superficie total de 73mÂ² cubierta de 57mÂ² y descubierta de 16mÂ² Tres dorm...'\n",
      "\n",
      "ğŸ’° Precios: Î¼=$160,807 | mediana=$139,100\n",
      "   Rango: $2,170 - $488,274\n",
      "\n",
      "ğŸ¯ BASELINE A SUPERAR (Consigna 4):\n",
      "   RMSE: $42,960 | RÂ²: 0.7284\n",
      "   Meta: Mejorar con features de texto (NLP)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” CONFIGURACIÃ“N Y CARGA DE DATOS - CONSIGNA 5\n",
    "print(\"ğŸ“‹ CONFIGURACIÃ“N INICIAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear directorio de modelos si no existe\n",
    "import os\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "    print(\" Directorio 'models' creado\")\n",
    "else:\n",
    "    print(\"ğŸ“ Directorio 'models' existe\")\n",
    "\n",
    "# Cargar dataset filtrado (del notebook 01) o aplicar filtros bÃ¡sicos\n",
    "print(\"\\nğŸ“‹ CARGANDO DATASET PARA ANÃLISIS NLP\")\n",
    "try:\n",
    "    df = pd.read_csv('train_bi_2025_filtered.csv')\n",
    "    print(\"âœ… Dataset filtrado cargado (del notebook 01)\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.read_csv('train_bi_2025.csv')\n",
    "    print(\"âš ï¸ Cargando dataset original - aplicando filtros bÃ¡sicos\")\n",
    "    \n",
    "    # Aplicar filtros bÃ¡sicos si no existe el filtrado\n",
    "    df = df.dropna(subset=['price', 'description'])\n",
    "    Q1, Q3 = df['price'].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: {df.shape[0]:,} Ã— {df.shape[1]} | {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Verificar columna de descripciÃ³n (es CRUCIAL para Consigna 5)\n",
    "if 'description' in df.columns:\n",
    "    desc_valid = df['description'].dropna()\n",
    "    print(f\"ğŸ“ Descripciones vÃ¡lidas: {len(desc_valid):,} ({len(desc_valid)/len(df)*100:.1f}%)\")\n",
    "    print(f\"ğŸ“ Longitud promedio: {desc_valid.str.len().mean():.0f} caracteres\")\n",
    "    \n",
    "    # Ejemplo de descripciÃ³n\n",
    "    sample_idx = desc_valid.index[0]\n",
    "    sample_desc = desc_valid.iloc[0]\n",
    "    sample_price = df.loc[sample_idx, 'price']\n",
    "    print(f\"\\nğŸ’­ Ejemplo - Precio: ${sample_price:,}\")\n",
    "    print(f\"   '{sample_desc[:120]}...'\")\n",
    "else:\n",
    "    raise ValueError(\"âŒ ERROR: Columna 'description' no encontrada - necesaria para Consigna 5\")\n",
    "\n",
    "# EstadÃ­sticas del precio (target)\n",
    "print(f\"\\nğŸ’° Precios: Î¼=${df['price'].mean():,.0f} | mediana=${df['price'].median():,.0f}\")\n",
    "print(f\"   Rango: ${df['price'].min():,.0f} - ${df['price'].max():,.0f}\")\n",
    "\n",
    "# Baseline desde notebooks anteriores (Consigna 4)\n",
    "BASELINE_RMSE = 42960  # LASSO optimizado del notebook 02\n",
    "BASELINE_R2 = 0.7284\n",
    "print(f\"\\nğŸ¯ BASELINE A SUPERAR (Consigna 4):\")\n",
    "print(f\"   RMSE: ${BASELINE_RMSE:,} | RÂ²: {BASELINE_R2:.4f}\")\n",
    "print(f\"   Meta: Mejorar con features de texto (NLP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba2e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” VERIFICACIÃ“N DE CALIDAD - CONSIGNA 5\n",
      "==================================================\n",
      "Registros con descripciÃ³n vÃ¡lida: 311,567/311,660 (100.0%)\n",
      "ğŸ“ Longitud promedio descripciÃ³n: 935 caracteres\n",
      "âœ… Dataset listo para anÃ¡lisis NLP: 311,567 propiedades\n",
      "\n",
      "ğŸ“ DistribuciÃ³n longitud descripciones:\n",
      "   P25: 448 | P50: 745 | P75: 1211\n",
      "   Min: 20 | Max: 12577\n",
      "ğŸ“ Longitud promedio descripciÃ³n: 935 caracteres\n",
      "âœ… Dataset listo para anÃ¡lisis NLP: 311,567 propiedades\n",
      "\n",
      "ğŸ“ DistribuciÃ³n longitud descripciones:\n",
      "   P25: 448 | P50: 745 | P75: 1211\n",
      "   Min: 20 | Max: 12577\n"
     ]
    }
   ],
   "source": [
    "# Verificar calidad de datos para anÃ¡lisis NLP\n",
    "print(\"ğŸ” VERIFICACIÃ“N DE CALIDAD - CONSIGNA 5\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "original_size = len(df)\n",
    "\n",
    "# Filtros especÃ­ficos para anÃ¡lisis NLP\n",
    "df_nlp = df[\n",
    "    (df['description'].notna()) &\n",
    "    (df['description'].str.len() >= 20) &  # Descripciones mÃ­nimas\n",
    "    (df['price'].notna()) &\n",
    "    (df['surface_total'].notna())\n",
    "].copy()\n",
    "\n",
    "final_size = len(df_nlp)\n",
    "retention = final_size / original_size * 100\n",
    "\n",
    "print(f\"Registros con descripciÃ³n vÃ¡lida: {final_size:,}/{original_size:,} ({retention:.1f}%)\")\n",
    "print(f\"ğŸ“ Longitud promedio descripciÃ³n: {df_nlp['description'].str.len().mean():.0f} caracteres\")\n",
    "print(f\"âœ… Dataset listo para anÃ¡lisis NLP: {final_size:,} propiedades\")\n",
    "\n",
    "# Mostrar distribuciÃ³n de longitudes\n",
    "desc_lengths = df_nlp['description'].str.len()\n",
    "print(f\"\\nğŸ“ DistribuciÃ³n longitud descripciones:\")\n",
    "print(f\"   P25: {desc_lengths.quantile(0.25):.0f} | P50: {desc_lengths.quantile(0.5):.0f} | P75: {desc_lengths.quantile(0.75):.0f}\")\n",
    "print(f\"   Min: {desc_lengths.min():.0f} | Max: {desc_lengths.max():.0f}\")\n",
    "\n",
    "df = df_nlp.copy()  # Usar el dataset filtrado para NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12734034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Procesando texto...\n",
      "âœ… Completado | ReducciÃ³n: 21.7%\n",
      "ğŸ“Š Original: 935 â†’ Procesado: 733 caracteres promedio\n",
      "\n",
      "ğŸ’­ Ejemplo de procesamiento:\n",
      "Original: 'Apto crÃ©dito PH en PB de 4 ambientes al frente Superficie total de 73mÂ² cubierta de 57mÂ² y descubier...'\n",
      "Procesado: 'apto crÃ©dito ambientes frente superficie total 73m cubierta 57m descubierta 16m tres dormitorios baÃ±...'\n",
      "âœ… Completado | ReducciÃ³n: 21.7%\n",
      "ğŸ“Š Original: 935 â†’ Procesado: 733 caracteres promedio\n",
      "\n",
      "ğŸ’­ Ejemplo de procesamiento:\n",
      "Original: 'Apto crÃ©dito PH en PB de 4 ambientes al frente Superficie total de 73mÂ² cubierta de 57mÂ² y descubier...'\n",
      "Procesado: 'apto crÃ©dito ambientes frente superficie total 73m cubierta 57m descubierta 16m tres dormitorios baÃ±...'\n"
     ]
    }
   ],
   "source": [
    "# Funciones de preprocesamiento optimizadas\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesamiento eficiente de texto inmobiliario\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', ' ', text)  # URLs/emails\n",
    "    text = re.sub(r'[^a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘Ãœ0-9\\s]', ' ', text)  # Caracteres especiales\n",
    "    return ' '.join(text.split())  # Normalizar espacios\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Elimina stopwords espaÃ±olas + tÃ©rminos inmobiliarios\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Stopwords inmobiliarias\n",
    "    real_estate_stopwords = spanish_stopwords.union({\n",
    "        'inmueble', 'propiedad', 'venta', 'alquiler', 'consultar', 'precio',\n",
    "        'contacto', 'llamar', 'whatsapp', 'telÃ©fono', 'celular'\n",
    "    })\n",
    "    \n",
    "    words = [word for word in text.split() \n",
    "             if word not in real_estate_stopwords and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "print(\"ğŸ”¤ Procesando texto...\")\n",
    "df['description_clean'] = df['description'].apply(preprocess_text)\n",
    "df['description_processed'] = df['description_clean'].apply(remove_stopwords)\n",
    "\n",
    "# Stats\n",
    "original_len = df['description'].str.len().mean()\n",
    "processed_len = df['description_processed'].str.len().mean()\n",
    "reduction = (1 - processed_len/original_len) * 100\n",
    "\n",
    "print(f\"âœ… Completado | ReducciÃ³n: {reduction:.1f}%\")\n",
    "print(f\"ğŸ“Š Original: {original_len:.0f} â†’ Procesado: {processed_len:.0f} caracteres promedio\")\n",
    "\n",
    "# Ejemplo\n",
    "idx = df.index[0]\n",
    "print(f\"\\nğŸ’­ Ejemplo de procesamiento:\")\n",
    "print(f\"Original: '{df.loc[idx, 'description'][:100]}...'\")\n",
    "print(f\"Procesado: '{df.loc[idx, 'description_processed'][:100]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0e6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PREPARANDO FEATURES Y DIVISIÃ“N DE DATOS\n",
      "==================================================\n",
      "ğŸ”¢ NumÃ©ricas: 8 | ğŸ·ï¸ CategÃ³ricas: 3\n",
      "âœ… 85 variables dummy creadas\n",
      "ğŸ“Š Features tradicionales: 93 variables\n",
      "\n",
      "ğŸ“Š DivisiÃ³n completada:\n",
      "   â€¢ Train: 249,253 muestras (80.0%)\n",
      "   â€¢ Test: 62,314 muestras (20.0%)\n",
      "   â€¢ Textos: 249,253 train / 62,314 test\n",
      "\n",
      "ğŸ¯ BASELINE A SUPERAR:\n",
      "   â€¢ RMSE: $42,960\n",
      "   â€¢ RÂ²: 0.7284\n",
      "   â€¢ Meta: Combinar features tradicionales + NLP\n",
      "âœ… 85 variables dummy creadas\n",
      "ğŸ“Š Features tradicionales: 93 variables\n",
      "\n",
      "ğŸ“Š DivisiÃ³n completada:\n",
      "   â€¢ Train: 249,253 muestras (80.0%)\n",
      "   â€¢ Test: 62,314 muestras (20.0%)\n",
      "   â€¢ Textos: 249,253 train / 62,314 test\n",
      "\n",
      "ğŸ¯ BASELINE A SUPERAR:\n",
      "   â€¢ RMSE: $42,960\n",
      "   â€¢ RÂ²: 0.7284\n",
      "   â€¢ Meta: Combinar features tradicionales + NLP\n"
     ]
    }
   ],
   "source": [
    "# PreparaciÃ³n de features tradicionales y divisiÃ³n de datos\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepara features tradicionales para modelos hÃ­bridos\"\"\"\n",
    "    exclude_cols = ['lat', 'lon', 'price', 'description', 'description_clean', 'description_processed']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['price'].copy()\n",
    "    \n",
    "    # One-hot encoding para categÃ³ricas\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"ğŸ”¢ NumÃ©ricas: {len(numerical_cols)} | ğŸ·ï¸ CategÃ³ricas: {len(categorical_cols)}\")\n",
    "    \n",
    "    if categorical_cols:\n",
    "        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "        print(f\"âœ… {X.shape[1] - len(numerical_cols)} variables dummy creadas\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Preparar features\n",
    "print(\"ğŸ”§ PREPARANDO FEATURES Y DIVISIÃ“N DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_traditional, y = prepare_features(df)\n",
    "print(f\"ğŸ“Š Features tradicionales: {X_traditional.shape[1]} variables\")\n",
    "\n",
    "# DivisiÃ³n train/test consistente con notebooks anteriores\n",
    "X_trad_train, X_trad_test, y_train, y_test = train_test_split(\n",
    "    X_traditional, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Textos con mismo split\n",
    "text_train = df.loc[X_trad_train.index, 'description_processed'].values\n",
    "text_test = df.loc[X_trad_test.index, 'description_processed'].values\n",
    "\n",
    "# Baseline a superar (de modelos tradicionales)\n",
    "BASELINE_RMSE = 42960  # LASSO de notebook 02\n",
    "BASELINE_R2 = 0.7284\n",
    "\n",
    "print(f\"\\nğŸ“Š DivisiÃ³n completada:\")\n",
    "print(f\"   â€¢ Train: {len(X_trad_train):,} muestras ({len(X_trad_train)/len(X_traditional)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Test: {len(X_trad_test):,} muestras ({len(X_trad_test)/len(X_traditional)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Textos: {len(text_train):,} train / {len(text_test):,} test\")\n",
    "\n",
    "print(f\"\\nğŸ¯ BASELINE A SUPERAR:\")\n",
    "print(f\"   â€¢ RMSE: ${BASELINE_RMSE:,}\")\n",
    "print(f\"   â€¢ RÂ²: {BASELINE_R2:.4f}\")\n",
    "print(f\"   â€¢ Meta: Combinar features tradicionales + NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7857ad",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Funciones de EvaluaciÃ³n y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe8a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ FunciÃ³n de evaluaciÃ³n NLP configurada\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n de evaluaciÃ³n optimizada para Consigna 5\n",
    "def evaluate_model_nlp(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"EvaluaciÃ³n completa con comparaciÃ³n vs baseline para modelos NLP\"\"\"\n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # MÃ©tricas\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # ComparaciÃ³n con baseline\n",
    "    rmse_improvement = BASELINE_RMSE - test_rmse\n",
    "    r2_improvement = test_r2 - BASELINE_R2\n",
    "    better_than_baseline = test_rmse < BASELINE_RMSE\n",
    "    overfitting_ratio = test_rmse / train_rmse\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"\\nğŸ“Š RESULTADOS {model_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"RMSE Train: ${train_rmse:,.0f} | Test: ${test_rmse:,.0f}\")\n",
    "    print(f\"RÂ² Train: {train_r2:.4f} | Test: {test_r2:.4f}\")\n",
    "    print(f\"MAE Test: ${test_mae:,.0f} | Overfitting: {overfitting_ratio:.3f}\")\n",
    "    \n",
    "    if better_than_baseline:\n",
    "        print(f\"âœ… SUPERA BASELINE: +${rmse_improvement:,.0f} RMSE | +{r2_improvement:.4f} RÂ²\")\n",
    "    else:\n",
    "        print(f\"âŒ No supera baseline: {rmse_improvement:,.0f} RMSE\")\n",
    "    \n",
    "    return {\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'overfitting_ratio': overfitting_ratio,\n",
    "        'rmse_improvement': rmse_improvement,\n",
    "        'r2_improvement': r2_improvement,\n",
    "        'better_than_baseline': better_than_baseline\n",
    "    }\n",
    "\n",
    "print(\"ğŸ› ï¸ FunciÃ³n de evaluaciÃ³n NLP configurada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267b240",
   "metadata": {},
   "source": [
    "## ğŸ“Š Consigna 5a: RepresentaciÃ³n Vectorial (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450090e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š CONSIGNA 5A: REPRESENTACIÃ“N VECTORIAL TF-IDF\n",
      "============================================================\n",
      "ğŸ”§ Configurando TF-IDF optimizado para inmuebles...\n",
      "ğŸ”„ Aplicando TF-IDF a descripciones...\n",
      "âœ… Matriz TF-IDF: 311,567 propiedades Ã— 5,000 tÃ©rminos\n",
      "ğŸ“Š Densidad de matriz: 1.85%\n",
      "ğŸ“ Vocabulario total: 5,000 tÃ©rminos Ãºnicos\n",
      "\n",
      "ğŸ” TOP 15 TÃ‰RMINOS MÃS FRECUENTES:\n",
      " 1. ambientes              9713.9\n",
      " 2. cocina                 8967.4\n",
      " 3. departamento           8711.6\n",
      " 4. balcon                 8660.0\n",
      " 5. bano                   8224.3\n",
      " 6. piso                   7910.2\n",
      " 7. comedor                7454.3\n",
      " 8. excelente              7126.4\n",
      " 9. frente                 7112.0\n",
      "10. edificio               7050.1\n",
      "11. living                 6900.8\n",
      "12. expensas               6668.0\n",
      "13. completo               6563.1\n",
      "14. dormitorio             6526.7\n",
      "15. pisos                  6336.0\n",
      "\n",
      "ğŸ“‰ Aplicando SVD para reducciÃ³n dimensional...\n",
      "âœ… Matriz TF-IDF: 311,567 propiedades Ã— 5,000 tÃ©rminos\n",
      "ğŸ“Š Densidad de matriz: 1.85%\n",
      "ğŸ“ Vocabulario total: 5,000 tÃ©rminos Ãºnicos\n",
      "\n",
      "ğŸ” TOP 15 TÃ‰RMINOS MÃS FRECUENTES:\n",
      " 1. ambientes              9713.9\n",
      " 2. cocina                 8967.4\n",
      " 3. departamento           8711.6\n",
      " 4. balcon                 8660.0\n",
      " 5. bano                   8224.3\n",
      " 6. piso                   7910.2\n",
      " 7. comedor                7454.3\n",
      " 8. excelente              7126.4\n",
      " 9. frente                 7112.0\n",
      "10. edificio               7050.1\n",
      "11. living                 6900.8\n",
      "12. expensas               6668.0\n",
      "13. completo               6563.1\n",
      "14. dormitorio             6526.7\n",
      "15. pisos                  6336.0\n",
      "\n",
      "ğŸ“‰ Aplicando SVD para reducciÃ³n dimensional...\n",
      "âœ… SVD: 300 componentes explican 0.5021 de la varianza\n",
      "\n",
      "ğŸ’¾ Guardando transformadores...\n",
      "âœ… TF-IDF y SVD guardados en models/\n",
      "\n",
      "ğŸ“Š RESUMEN REPRESENTACIÃ“N VECTORIAL:\n",
      "   ğŸ”¤ Texto original â†’ TF-IDF: 5,000 dimensiones\n",
      "   ğŸ“‰ TF-IDF â†’ SVD: 300 dimensiones\n",
      "   ğŸ’¾ Transformadores guardados para reproducibilidad\n",
      "   âœ… Consigna 5a completada\n",
      "âœ… SVD: 300 componentes explican 0.5021 de la varianza\n",
      "\n",
      "ğŸ’¾ Guardando transformadores...\n",
      "âœ… TF-IDF y SVD guardados en models/\n",
      "\n",
      "ğŸ“Š RESUMEN REPRESENTACIÃ“N VECTORIAL:\n",
      "   ğŸ”¤ Texto original â†’ TF-IDF: 5,000 dimensiones\n",
      "   ğŸ“‰ TF-IDF â†’ SVD: 300 dimensiones\n",
      "   ğŸ’¾ Transformadores guardados para reproducibilidad\n",
      "   âœ… Consigna 5a completada\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“š CONSIGNA 5A: REPRESENTACIÃ“N VECTORIAL TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configurar TF-IDF para descripciones inmobiliarias en espaÃ±ol\n",
    "print(\"ğŸ”§ Configurando TF-IDF optimizado para inmuebles...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,       # LÃ­mite para eficiencia de memoria\n",
    "    stop_words=list(spanish_stopwords),  # Lista de stopwords en espaÃ±ol\n",
    "    ngram_range=(1, 2),      # Unigramas y bigramas\n",
    "    min_df=5,                # MÃ­nimo 5 documentos para incluir tÃ©rmino\n",
    "    max_df=0.8,              # MÃ¡ximo 80% documentos (eliminar muy comunes)\n",
    "    strip_accents='unicode', # Normalizar acentos\n",
    "    lowercase=True,          # Convertir a minÃºsculas\n",
    "    token_pattern=r'[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃÃ‰ÃÃ“ÃšÃ±Ã‘]{2,}'  # Solo palabras 2+ caracteres\n",
    ")\n",
    "\n",
    "# Aplicar TF-IDF a las descripciones procesadas\n",
    "print(\"ğŸ”„ Aplicando TF-IDF a descripciones...\")\n",
    "X_text_tfidf = tfidf.fit_transform(df['description_processed'])\n",
    "\n",
    "print(f\"âœ… Matriz TF-IDF: {X_text_tfidf.shape[0]:,} propiedades Ã— {X_text_tfidf.shape[1]:,} tÃ©rminos\")\n",
    "print(f\"ğŸ“Š Densidad de matriz: {X_text_tfidf.nnz / (X_text_tfidf.shape[0] * X_text_tfidf.shape[1]) * 100:.2f}%\")\n",
    "\n",
    "# AnÃ¡lisis de vocabulario mÃ¡s importante\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"ğŸ“ Vocabulario total: {len(feature_names):,} tÃ©rminos Ãºnicos\")\n",
    "\n",
    "# TÃ©rminos mÃ¡s frecuentes globalmente\n",
    "term_freq = np.array(X_text_tfidf.sum(axis=0)).flatten()\n",
    "top_indices = np.argsort(term_freq)[-15:]\n",
    "\n",
    "print(f\"\\nğŸ” TOP 15 TÃ‰RMINOS MÃS FRECUENTES:\")\n",
    "for i, idx in enumerate(reversed(top_indices)):\n",
    "    term = feature_names[idx]\n",
    "    freq = term_freq[idx]\n",
    "    print(f\"{i+1:2d}. {term:20s} {freq:8.1f}\")\n",
    "\n",
    "# ReducciÃ³n dimensional con SVD (para eficiencia computacional)\n",
    "print(f\"\\nğŸ“‰ Aplicando SVD para reducciÃ³n dimensional...\")\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_text_svd = svd.fit_transform(X_text_tfidf)\n",
    "\n",
    "explained_var = svd.explained_variance_ratio_.sum()\n",
    "print(f\"âœ… SVD: {X_text_svd.shape[1]} componentes explican {explained_var:.4f} de la varianza\")\n",
    "\n",
    "# Guardar transformadores para uso posterior\n",
    "print(f\"\\nğŸ’¾ Guardando transformadores...\")\n",
    "joblib.dump(tfidf, 'models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(svd, 'models/svd_transformer.pkl')\n",
    "print(f\"âœ… TF-IDF y SVD guardados en models/\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RESUMEN REPRESENTACIÃ“N VECTORIAL:\")\n",
    "print(f\"   ğŸ”¤ Texto original â†’ TF-IDF: {X_text_tfidf.shape[1]:,} dimensiones\")\n",
    "print(f\"   ğŸ“‰ TF-IDF â†’ SVD: {X_text_svd.shape[1]} dimensiones\")\n",
    "print(f\"   ğŸ’¾ Transformadores guardados para reproducibilidad\")\n",
    "print(f\"   âœ… Consigna 5a completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af40fa4",
   "metadata": {},
   "source": [
    "## ğŸ”— Consigna 5b: Incorporar RepresentaciÃ³n al Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "830954be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— CONSIGNA 5B: INCORPORAR REPRESENTACIÃ“N AL DATASET\n",
      "============================================================\n",
      "ğŸ”§ Preparando features tradicionales...\n",
      "ğŸ“Š Features tradicionales: 11 variables\n",
      "   Variables: ['l2', 'l3', 'prop_type', 'rooms', 'bathrooms']...\n",
      " Variables numÃ©ricas: 8\n",
      "ğŸ·ï¸ Variables categÃ³ricas: 3\n",
      "   CategÃ³ricas: ['l2', 'l3', 'prop_type']\n",
      "âœ… One-hot encoding aplicado: 93 variables finales\n",
      "\n",
      "âš–ï¸ Escalando features tradicionales...\n",
      "âœ… One-hot encoding aplicado: 93 variables finales\n",
      "\n",
      "âš–ï¸ Escalando features tradicionales...\n",
      "âœ… Features tradicionales escaladas: (311567, 93)\n",
      "\n",
      "ğŸ”— COMBINANDO DATASETS (Consigna 5b):\n",
      "   ğŸ“Š Features tradicionales: 93 dimensiones\n",
      "   ğŸ“ Features de texto (SVD): 300 dimensiones\n",
      "âœ… Features tradicionales escaladas: (311567, 93)\n",
      "\n",
      "ğŸ”— COMBINANDO DATASETS (Consigna 5b):\n",
      "   ğŸ“Š Features tradicionales: 93 dimensiones\n",
      "   ğŸ“ Features de texto (SVD): 300 dimensiones\n",
      "âœ… Dataset hÃ­brido: 393 features totales\n",
      "   ComposiciÃ³n: 93 tradicionales + 300 texto\n",
      "\n",
      "ğŸ“‹ Dataset final para Consigna 5c:\n",
      "   ğŸ“ Dimensiones: 311,567 muestras Ã— 393 features\n",
      "   ğŸ¯ Target: precio de inmuebles\n",
      "   ğŸ’¾ Features tradicionales y de texto combinadas\n",
      "\n",
      "ğŸ”€ DivisiÃ³n train/test...\n",
      "âœ… Dataset hÃ­brido: 393 features totales\n",
      "   ComposiciÃ³n: 93 tradicionales + 300 texto\n",
      "\n",
      "ğŸ“‹ Dataset final para Consigna 5c:\n",
      "   ğŸ“ Dimensiones: 311,567 muestras Ã— 393 features\n",
      "   ğŸ¯ Target: precio de inmuebles\n",
      "   ğŸ’¾ Features tradicionales y de texto combinadas\n",
      "\n",
      "ğŸ”€ DivisiÃ³n train/test...\n",
      "âœ… DivisiÃ³n completada:\n",
      "   ğŸ“š Train: 249,253 muestras (80.0%)\n",
      "   ğŸ§ª Test: 62,314 muestras (20.0%)\n",
      "\n",
      "ğŸ’¾ Scaler guardado para reproducibilidad\n",
      "âœ… Consigna 5b completada - Dataset hÃ­brido listo\n",
      "âœ… DivisiÃ³n completada:\n",
      "   ğŸ“š Train: 249,253 muestras (80.0%)\n",
      "   ğŸ§ª Test: 62,314 muestras (20.0%)\n",
      "\n",
      "ğŸ’¾ Scaler guardado para reproducibilidad\n",
      "âœ… Consigna 5b completada - Dataset hÃ­brido listo\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”— CONSIGNA 5B: INCORPORAR REPRESENTACIÃ“N AL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar features tradicionales (sin texto y coordenadas)\n",
    "print(\"ğŸ”§ Preparando features tradicionales...\")\n",
    "exclude_cols = ['lat', 'lon', 'price', 'description', 'description_clean', 'description_processed']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X_traditional = df[feature_cols].copy()\n",
    "y = df['price'].copy()\n",
    "\n",
    "print(f\"ğŸ“Š Features tradicionales: {len(feature_cols)} variables\")\n",
    "print(f\"   Variables: {feature_cols[:5]}...\" if len(feature_cols) > 5 else f\"   Variables: {feature_cols}\")\n",
    "\n",
    "# CodificaciÃ³n de variables categÃ³ricas\n",
    "categorical_cols = X_traditional.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_traditional.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\" Variables numÃ©ricas: {len(numerical_cols)}\")\n",
    "print(f\"ğŸ·ï¸ Variables categÃ³ricas: {len(categorical_cols)}\")\n",
    "\n",
    "if categorical_cols:\n",
    "    print(f\"   CategÃ³ricas: {categorical_cols}\")\n",
    "    X_traditional = pd.get_dummies(X_traditional, columns=categorical_cols, drop_first=True)\n",
    "    print(f\"âœ… One-hot encoding aplicado: {X_traditional.shape[1]} variables finales\")\n",
    "\n",
    "# Escalado de features tradicionales\n",
    "print(f\"\\nâš–ï¸ Escalando features tradicionales...\")\n",
    "scaler = StandardScaler()\n",
    "X_traditional_scaled = scaler.fit_transform(X_traditional)\n",
    "print(f\"âœ… Features tradicionales escaladas: {X_traditional_scaled.shape}\")\n",
    "\n",
    "# CONSIGNA 5B: Combinar features tradicionales + representaciÃ³n de texto\n",
    "print(f\"\\nğŸ”— COMBINANDO DATASETS (Consigna 5b):\")\n",
    "print(f\"   ğŸ“Š Features tradicionales: {X_traditional_scaled.shape[1]} dimensiones\")\n",
    "print(f\"   ğŸ“ Features de texto (SVD): {X_text_svd.shape[1]} dimensiones\")\n",
    "\n",
    "# Crear dataset hÃ­brido completo\n",
    "X_hybrid = np.hstack([X_traditional_scaled, X_text_svd])\n",
    "print(f\"âœ… Dataset hÃ­brido: {X_hybrid.shape[1]} features totales\")\n",
    "print(f\"   ComposiciÃ³n: {X_traditional_scaled.shape[1]} tradicionales + {X_text_svd.shape[1]} texto\")\n",
    "\n",
    "# Crear nombres de features para interpretabilidad\n",
    "traditional_names = [f\"trad_{col}\" for col in X_traditional.columns]\n",
    "text_names = [f\"texto_dim_{i}\" for i in range(X_text_svd.shape[1])]\n",
    "feature_names = traditional_names + text_names\n",
    "\n",
    "print(f\"\\nğŸ“‹ Dataset final para Consigna 5c:\")\n",
    "print(f\"   ğŸ“ Dimensiones: {X_hybrid.shape[0]:,} muestras Ã— {X_hybrid.shape[1]} features\")\n",
    "print(f\"   ğŸ¯ Target: precio de inmuebles\")\n",
    "print(f\"   ğŸ’¾ Features tradicionales y de texto combinadas\")\n",
    "\n",
    "# DivisiÃ³n train/test consistente\n",
    "print(f\"\\nğŸ”€ DivisiÃ³n train/test...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_hybrid, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"âœ… DivisiÃ³n completada:\")\n",
    "print(f\"   ğŸ“š Train: {X_train.shape[0]:,} muestras ({X_train.shape[0]/len(X_hybrid)*100:.1f}%)\")\n",
    "print(f\"   ğŸ§ª Test: {X_test.shape[0]:,} muestras ({X_test.shape[0]/len(X_hybrid)*100:.1f}%)\")\n",
    "\n",
    "# Guardar scaler para reproducibilidad\n",
    "joblib.dump(scaler, 'models/scaler_hybrid.pkl')\n",
    "print(f\"\\nğŸ’¾ Scaler guardado para reproducibilidad\")\n",
    "print(f\"âœ… Consigna 5b completada - Dataset hÃ­brido listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54842f39",
   "metadata": {},
   "source": [
    "## ğŸŒ² Consigna 5c.1: Random Forest + HiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3be838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ² CONSIGNA 5C.1: RANDOM FOREST + BÃšSQUEDA HIPERPARÃMETROS\n",
      "======================================================================\n",
      "ğŸ” Configurando bÃºsqueda de hiperparÃ¡metros...\n",
      "ğŸ“Š Espacio de bÃºsqueda: 108 combinaciones\n",
      "ğŸ“ n_estimators se usa como resource (50-300 Ã¡rboles progresivamente)\n",
      "ğŸ”„ Ejecutando HalvingRandomSearchCV (eficiente para dataset grande)...\n",
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 50\n",
      "max_resources_: 300\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 50\n",
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 150\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 150\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "\n",
      "ğŸ† MEJORES HIPERPARÃMETROS ENCONTRADOS:\n",
      "   min_samples_split: 5\n",
      "   min_samples_leaf: 2\n",
      "   max_features: 0.6\n",
      "   max_depth: 20\n",
      "   n_estimators: 150\n",
      "\n",
      "ğŸ¯ Mejor score CV: 1032785570 RMSE\n",
      "ğŸŒ² n_estimators optimizado: 150\n",
      "\n",
      "ğŸ“Š EVALUACIÃ“N RANDOM FOREST OPTIMIZADO:\n",
      "==================================================\n",
      "RMSE Train: $17,730 | Test: $29,900\n",
      "RÂ² Train: 0.9545 | Test: 0.8701\n",
      "MAE Test: $19,455\n",
      "Overfitting ratio: 1.686\n",
      "âœ… SUPERA BASELINE por $13,060 RMSE (30.4%)\n",
      "   Mejora RÂ²: +0.1417\n",
      "\n",
      "ğŸ“ˆ ANÃLISIS DE IMPORTANCIA DE FEATURES:\n",
      "\n",
      "ğŸ” TOP 15 FEATURES MÃS IMPORTANTES:\n",
      " 1. ğŸ“Š Trad trad_surface_covered      0.3413\n",
      " 2. ğŸ“Š Trad trad_surface_total        0.2034\n",
      " 3. ğŸ“Š Trad trad_bathrooms            0.0631\n",
      " 4. ğŸ“ Texto texto_dim_12              0.0483\n",
      " 5. ğŸ“Š Trad trad_l3_Palermo           0.0259\n",
      " 6. ğŸ“ Texto texto_dim_11              0.0246\n",
      " 7. ğŸ“Š Trad trad_rooms                0.0184\n",
      " 8. ğŸ“Š Trad trad_l3_Puerto Madero     0.0173\n",
      " 9. ğŸ“Š Trad trad_l3_Belgrano          0.0119\n",
      "10. ğŸ“Š Trad trad_l3_Recoleta          0.0112\n",
      "11. ğŸ“ Texto texto_dim_10              0.0108\n",
      "12. ğŸ“ Texto texto_dim_5               0.0079\n",
      "13. ğŸ“Š Trad trad_prop_type_Departamen 0.0076\n",
      "14. ğŸ“Š Trad trad_created_year         0.0073\n",
      "15. ğŸ“Š Trad trad_l3_Barrio Norte      0.0043\n",
      "\n",
      "ğŸ“Š IMPORTANCIA PROMEDIO POR TIPO:\n",
      "   ğŸ“ Features de texto: 0.000924\n",
      "   ğŸ“Š Features tradicionales: 0.007772\n",
      "   ğŸ”— ContribuciÃ³n texto: 0.277 (27.7%)\n",
      "\n",
      "ğŸ’¾ Modelo Random Forest optimizado guardado\n",
      "ğŸ’¾ Guardando: Random Forest NLP\n",
      "âœ… Guardado en resultados_modelos_nlp.yaml\n",
      "âœ… Random Forest con NLP completado - Consigna 5c.1\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸŒ² CONSIGNA 5C.1: RANDOM FOREST + BÃšSQUEDA HIPERPARÃMETROS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Importar HalvingRandomSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# Definir espacio de hiperparÃ¡metros para Random Forest (SIN n_estimators)\n",
    "print(\"ğŸ” Configurando bÃºsqueda de hiperparÃ¡metros...\")\n",
    "param_grid_rf = {\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 0.6, 0.8]\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“Š Espacio de bÃºsqueda: {np.prod([len(v) for v in param_grid_rf.values()]):,} combinaciones\")\n",
    "print(\"ğŸ“ n_estimators se usa como resource (50-300 Ã¡rboles progresivamente)\")\n",
    "\n",
    "# Crear modelo base (n_jobs=1 para evitar nested parallelism)\n",
    "print(\"âš™ï¸ ConfiguraciÃ³n anti-nested parallelism: RF con n_jobs=1, bÃºsqueda con n_jobs=-1\")\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=1)\n",
    "\n",
    "# BÃºsqueda de hiperparÃ¡metros con HalvingRandomSearchCV optimizada\n",
    "print(\"ğŸ”„ Ejecutando HalvingRandomSearchCV optimizada...\")\n",
    "rf_search = HalvingRandomSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_grid_rf,\n",
    "    factor=3,                # Keep the top 1/3 each round\n",
    "    resource='n_estimators', # Use n_estimators as \"budget\"\n",
    "    max_resources=300,       # max trees\n",
    "    min_resources=50,        # start with 50 trees\n",
    "    cv=2,                    # 2-fold CV eficiente para primer filtrado\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,               # ParalelizaciÃ³n solo en CV, no en RF\n",
    "    random_state=42,         # Reproducibilidad completa\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Entrenar y buscar mejores hiperparÃ¡metros\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "# Monitoreo iterativo post-entrenamiento\n",
    "print(f\"\\nğŸ“ˆ MONITOREO DE BÃšSQUEDA SUCESIVA:\")\n",
    "print(f\"   ğŸ”„ Iteraciones totales: {rf_search.n_iterations_}\")\n",
    "print(f\"   ğŸ‘¥ Candidatos evaluados: {len(rf_search.cv_results_['params'])}\")\n",
    "print(f\"   ğŸ¯ Recursos finales utilizados: {rf_search.n_resources_}\")\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_rf = rf_search.best_estimator_\n",
    "print(f\"\\nğŸ† MEJORES HIPERPARÃMETROS ENCONTRADOS:\")\n",
    "for param, value in rf_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Mejor score CV: {-rf_search.best_score_:.0f} RMSE\")\n",
    "print(f\"ğŸŒ² n_estimators optimizado: {best_rf.n_estimators}\")\n",
    "\n",
    "# EvaluaciÃ³n completa del mejor modelo\n",
    "print(f\"\\nğŸ“Š EVALUACIÃ“N RANDOM FOREST OPTIMIZADO:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar RF final con paralelizaciÃ³n completa para predicciÃ³n\n",
    "best_rf.set_params(n_jobs=-1)  # Ahora sÃ­ usar todos los cores para predicciÃ³n\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = best_rf.predict(X_train)\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "\n",
    "# MÃ©tricas\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "# ComparaciÃ³n con baseline\n",
    "rmse_improvement = BASELINE_RMSE - test_rmse\n",
    "r2_improvement = test_r2 - BASELINE_R2\n",
    "overfitting_ratio = test_rmse / train_rmse\n",
    "\n",
    "print(f\"RMSE Train: ${train_rmse:,.0f} | Test: ${test_rmse:,.0f}\")\n",
    "print(f\"RÂ² Train: {train_r2:.4f} | Test: {test_r2:.4f}\")\n",
    "print(f\"MAE Test: ${test_mae:,.0f}\")\n",
    "print(f\"Overfitting ratio: {overfitting_ratio:.3f}\")\n",
    "\n",
    "if test_rmse < BASELINE_RMSE:\n",
    "    print(f\"âœ… SUPERA BASELINE por ${rmse_improvement:,.0f} RMSE ({rmse_improvement/BASELINE_RMSE*100:.1f}%)\")\n",
    "    print(f\"   Mejora RÂ²: +{r2_improvement:.4f}\")\n",
    "else:\n",
    "    print(f\"âŒ No supera baseline: +${rmse_improvement:,.0f} RMSE\")\n",
    "\n",
    "# AnÃ¡lisis de feature importance\n",
    "print(f\"\\nğŸ“ˆ ANÃLISIS DE IMPORTANCIA DE FEATURES:\")\n",
    "importances = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 15 features mÃ¡s importantes\n",
    "top_15 = feature_importance_df.head(15)\n",
    "print(f\"\\nğŸ” TOP 15 FEATURES MÃS IMPORTANTES:\")\n",
    "for i, (_, row) in enumerate(top_15.iterrows(), 1):\n",
    "    feature_type = \"ğŸ“ Texto\" if row['feature'].startswith('texto_') else \"ğŸ“Š Trad\"\n",
    "    print(f\"{i:2d}. {feature_type} {row['feature'][:25]:25s} {row['importance']:.4f}\")\n",
    "\n",
    "# Importancia promedio por tipo de feature\n",
    "text_features = feature_importance_df[feature_importance_df['feature'].str.startswith('texto_')]\n",
    "trad_features = feature_importance_df[feature_importance_df['feature'].str.startswith('trad_')]\n",
    "\n",
    "print(f\"\\nğŸ“Š IMPORTANCIA PROMEDIO POR TIPO:\")\n",
    "print(f\"   ğŸ“ Features de texto: {text_features['importance'].mean():.6f}\")\n",
    "print(f\"   ğŸ“Š Features tradicionales: {trad_features['importance'].mean():.6f}\")\n",
    "print(f\"   ğŸ”— ContribuciÃ³n texto: {text_features['importance'].sum():.3f} ({text_features['importance'].sum()*100:.1f}%)\")\n",
    "\n",
    "# InformaciÃ³n de eficiencia de bÃºsqueda\n",
    "efficiency_ratio = rf_search.n_resources_ / (len(param_grid_rf['max_depth']) * \n",
    "                                           len(param_grid_rf['min_samples_split']) * \n",
    "                                           len(param_grid_rf['min_samples_leaf']) * \n",
    "                                           len(param_grid_rf['max_features']) * 300)\n",
    "print(f\"\\nâš¡ EFICIENCIA DE BÃšSQUEDA:\")\n",
    "print(f\"   ğŸ¯ Ratio recursos/bÃºsqueda exhaustiva: {efficiency_ratio:.3f}\")\n",
    "print(f\"   â±ï¸ ReducciÃ³n computacional: ~{(1-efficiency_ratio)*100:.1f}%\")\n",
    "\n",
    "# Guardar modelo optimizado\n",
    "joblib.dump(best_rf, 'models/random_forest_nlp_optimized.pkl')\n",
    "print(f\"\\nğŸ’¾ Modelo Random Forest optimizado guardado\")\n",
    "\n",
    "# Guardar resultados para comparaciÃ³n\n",
    "rf_results = {\n",
    "    'modelo': 'Random Forest NLP',\n",
    "    'hiperparametros': rf_search.best_params_,\n",
    "    'cv_score': float(-rf_search.best_score_),\n",
    "    'train_rmse': float(train_rmse),\n",
    "    'test_rmse': float(test_rmse),\n",
    "    'train_r2': float(train_r2),\n",
    "    'test_r2': float(test_r2),\n",
    "    'test_mae': float(test_mae),\n",
    "    'overfitting_ratio': float(overfitting_ratio),\n",
    "    'rmse_improvement': float(rmse_improvement),\n",
    "    'r2_improvement': float(r2_improvement),\n",
    "    'better_than_baseline': bool(test_rmse < BASELINE_RMSE),\n",
    "    'top_features': top_15.head(5).to_dict('records'),  # Top 5 para resumen\n",
    "    'n_estimators_final': int(best_rf.n_estimators),\n",
    "    'search_iterations': int(rf_search.n_iterations_),\n",
    "    'efficiency_ratio': float(efficiency_ratio)\n",
    "}\n",
    "\n",
    "# GUARDAR EN YAML\n",
    "save_result_to_yaml(rf_results)\n",
    "\n",
    "print(f\"âœ… Random Forest con NLP completado - Consigna 5c.1\")\n",
    "print(f\"ğŸš€ OptimizaciÃ³n HalvingRandomSearchCV: {efficiency_ratio:.1%} recursos vs bÃºsqueda exhaustiva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad65bf",
   "metadata": {},
   "source": [
    "## âš¡ Consigna 5c.2: XGBoost + HiperparÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e2f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ CONSIGNA 5C.2: XGBOOST + BÃšSQUEDA HIPERPARÃMETROS\n",
      "======================================================================\n",
      "ğŸ” Configurando bÃºsqueda de hiperparÃ¡metros XGBoost...\n",
      "ğŸ“Š Espacio de bÃºsqueda: 324 combinaciones\n",
      "ğŸ“ n_estimators se usa como resource (100-500 estimadores progresivamente)\n",
      "âš™ï¸ ConfiguraciÃ³n anti-nested parallelism: XGB con n_jobs=1, bÃºsqueda con n_jobs=-1\n",
      "ğŸ”„ Ejecutando HalvingRandomSearchCV optimizada para XGBoost...\n",
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 100\n",
      "max_resources_: 500\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 5\n",
      "n_resources: 100\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 300\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 300\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "\n",
      "ğŸ† MEJORES HIPERPARÃMETROS XGBOOST:\n",
      "   subsample: 0.9\n",
      "   reg_lambda: 2.0\n",
      "   reg_alpha: 0.1\n",
      "   max_depth: 8\n",
      "   learning_rate: 0.15\n",
      "   colsample_bytree: 0.8\n",
      "   n_estimators: 300\n",
      "\n",
      "ğŸ¯ Mejor score CV: 769958144 RMSE\n",
      "\n",
      "ğŸ“Š EVALUACIÃ“N XGBOOST OPTIMIZADO:\n",
      "==================================================\n",
      "RMSE Train: $17,808 | Test: $26,471\n",
      "RÂ² Train: 0.9541 | Test: 0.8982\n",
      "MAE Test: $17,284\n",
      "Overfitting ratio: 1.486\n",
      "âœ… SUPERA BASELINE por $16,489 RMSE (38.4%)\n",
      "   Mejora RÂ²: +0.1698\n",
      "\n",
      "ğŸ“ˆ ANÃLISIS DE IMPORTANCIA XGBoost:\n",
      "\n",
      "TOP 15 FEATURES MÃS IMPORTANTES (XGBoost):\n",
      " 1. ğŸ“Š Trad trad_l3_Puerto Madero     0.0614\n",
      " 2. ğŸ“Š Trad trad_surface_covered      0.0554\n",
      " 3. ğŸ“Š Trad trad_l3_Palermo           0.0430\n",
      " 4. ğŸ“Š Trad trad_l3_Belgrano          0.0379\n",
      " 5. ğŸ“Š Trad trad_surface_total        0.0303\n",
      " 6. ğŸ“Š Trad trad_l3_NuÃ±ez             0.0284\n",
      " 7. ğŸ“Š Trad trad_l3_Recoleta          0.0274\n",
      " 8. ğŸ“ Texto texto_dim_12              0.0260\n",
      " 9. ğŸ“Š Trad trad_bathrooms            0.0245\n",
      "10. ğŸ“Š Trad trad_l3_Barrio Norte      0.0217\n",
      "11. ğŸ“Š Trad trad_l3_Las CaÃ±itas       0.0209\n",
      "12. ğŸ“Š Trad trad_l3_Balvanera         0.0192\n",
      "13. ğŸ“Š Trad trad_l3_Villa Lugano      0.0174\n",
      "14. ğŸ“Š Trad trad_l3_Once              0.0174\n",
      "15. ğŸ“Š Trad trad_prop_type_PH         0.0171\n",
      "\n",
      " IMPORTANCIA PROMEDIO POR TIPO (XGBoost):\n",
      "   ğŸ“ Features de texto: 0.001031\n",
      "   ğŸ“Š Features tradicionales: 0.007428\n",
      "   ğŸ”— ContribuciÃ³n texto: 0.309 (30.9%)\n",
      "\n",
      "ğŸ’¾ Modelo XGBoost optimizado guardado\n",
      "ğŸ’¾ Guardando: XGBoost NLP\n",
      "âœ… Guardado en resultados_modelos_nlp.yaml\n",
      "âœ… XGBoost con NLP completado - Consigna 5c.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "print(\"âš¡ CONSIGNA 5C.2: XGBOOST + BÃšSQUEDA HIPERPARÃMETROS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Definir espacio de hiperparÃ¡metros para XGBoost (SIN n_estimators)\n",
    "print(\"ğŸ” Configurando bÃºsqueda de hiperparÃ¡metros XGBoost...\")\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9],\n",
    "    'reg_alpha': [0.01, 0.1, 0.5],\n",
    "    'reg_lambda': [0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“Š Espacio de bÃºsqueda: {np.prod([len(v) for v in param_grid_xgb.values()]):,} combinaciones\")\n",
    "print(\"ğŸ“ n_estimators se usa como resource (100-500 estimadores progresivamente)\")\n",
    "\n",
    "# Crear modelo base XGBoost (n_jobs=1 para evitar nested parallelism)\n",
    "print(\"âš™ï¸ ConfiguraciÃ³n anti-nested parallelism: XGB con n_jobs=1, bÃºsqueda con n_jobs=-1\")\n",
    "xgb_base = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=1,                # Anti-nested parallelism\n",
    "    verbosity=0,\n",
    "    tree_method='hist',\n",
    "    predictor='cpu_predictor'\n",
    ")\n",
    "\n",
    "# BÃºsqueda de hiperparÃ¡metros con HalvingRandomSearchCV optimizada\n",
    "print(\"ğŸ”„ Ejecutando HalvingRandomSearchCV optimizada para XGBoost...\")\n",
    "xgb_search = HalvingRandomSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_grid_xgb,\n",
    "    factor=3,                # Keep the top 1/3 each round\n",
    "    resource='n_estimators', # Use n_estimators as \"budget\"\n",
    "    max_resources=500,       # max estimators\n",
    "    min_resources=100,       # start with 100 estimators\n",
    "    cv=2,                    # 2-fold CV eficiente para primer filtrado\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,               # ParalelizaciÃ³n solo en CV, no en XGB\n",
    "    random_state=42,         # Reproducibilidad completa\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Entrenar y buscar mejores hiperparÃ¡metros\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "# Monitoreo iterativo post-entrenamiento\n",
    "print(f\"\\nğŸ“ˆ MONITOREO DE BÃšSQUEDA SUCESIVA XGBOOST:\")\n",
    "print(f\"   ğŸ”„ Iteraciones totales: {xgb_search.n_iterations_}\")\n",
    "print(f\"   ğŸ‘¥ Candidatos evaluados: {len(xgb_search.cv_results_['params'])}\")\n",
    "print(f\"   ğŸ¯ Recursos finales utilizados: {xgb_search.n_resources_}\")\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "print(f\"\\nğŸ† MEJORES HIPERPARÃMETROS XGBOOST:\")\n",
    "for param, value in xgb_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Mejor score CV: {-xgb_search.best_score_:.0f} RMSE\")\n",
    "print(f\"âš¡ n_estimators optimizado: {best_xgb.n_estimators}\")\n",
    "\n",
    "# EvaluaciÃ³n completa del mejor modelo\n",
    "print(f\"\\nğŸ“Š EVALUACIÃ“N XGBOOST OPTIMIZADO:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar XGB final con paralelizaciÃ³n completa para predicciÃ³n\n",
    "best_xgb.set_params(n_jobs=-1)  # Ahora sÃ­ usar todos los cores para predicciÃ³n\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_xgb = best_xgb.predict(X_train)\n",
    "y_pred_test_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# MÃ©tricas\n",
    "train_rmse_xgb = np.sqrt(mean_squared_error(y_train, y_pred_train_xgb))\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_test_xgb))\n",
    "train_r2_xgb = r2_score(y_train, y_pred_train_xgb)\n",
    "test_r2_xgb = r2_score(y_test, y_pred_test_xgb)\n",
    "test_mae_xgb = mean_absolute_error(y_test, y_pred_test_xgb)\n",
    "\n",
    "# ComparaciÃ³n con baseline\n",
    "rmse_improvement_xgb = BASELINE_RMSE - test_rmse_xgb\n",
    "r2_improvement_xgb = test_r2_xgb - BASELINE_R2\n",
    "overfitting_ratio_xgb = test_rmse_xgb / train_rmse_xgb\n",
    "\n",
    "print(f\"RMSE Train: ${train_rmse_xgb:,.0f} | Test: ${test_rmse_xgb:,.0f}\")\n",
    "print(f\"RÂ² Train: {train_r2_xgb:.4f} | Test: {test_r2_xgb:.4f}\")\n",
    "print(f\"MAE Test: ${test_mae_xgb:,.0f}\")\n",
    "print(f\"Overfitting ratio: {overfitting_ratio_xgb:.3f}\")\n",
    "\n",
    "if test_rmse_xgb < BASELINE_RMSE:\n",
    "    print(f\"âœ… SUPERA BASELINE por ${rmse_improvement_xgb:,.0f} RMSE ({rmse_improvement_xgb/BASELINE_RMSE*100:.1f}%)\")\n",
    "    print(f\"   Mejora RÂ²: +{r2_improvement_xgb:.4f}\")\n",
    "else:\n",
    "    print(f\"âŒ No supera baseline: +${rmse_improvement_xgb:,.0f} RMSE\")\n",
    "\n",
    "# AnÃ¡lisis de feature importance XGBoost\n",
    "print(f\"\\nğŸ“ˆ ANÃLISIS DE IMPORTANCIA XGBoost:\")\n",
    "xgb_importances = best_xgb.feature_importances_\n",
    "xgb_feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': xgb_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 15 features mÃ¡s importantes\n",
    "xgb_top_15 = xgb_feature_importance_df.head(15)\n",
    "print(f\"\\nTOP 15 FEATURES MÃS IMPORTANTES (XGBoost):\")\n",
    "for i, (_, row) in enumerate(xgb_top_15.iterrows(), 1):\n",
    "    feature_type = \"ğŸ“ Texto\" if row['feature'].startswith('texto_') else \"ğŸ“Š Trad\"\n",
    "    print(f\"{i:2d}. {feature_type} {row['feature'][:25]:25s} {row['importance']:.4f}\")\n",
    "\n",
    "# Importancia promedio por tipo de feature\n",
    "xgb_text_features = xgb_feature_importance_df[xgb_feature_importance_df['feature'].str.startswith('texto_')]\n",
    "xgb_trad_features = xgb_feature_importance_df[xgb_feature_importance_df['feature'].str.startswith('trad_')]\n",
    "\n",
    "print(f\"\\n IMPORTANCIA PROMEDIO POR TIPO (XGBoost):\")\n",
    "print(f\"   ğŸ“ Features de texto: {xgb_text_features['importance'].mean():.6f}\")\n",
    "print(f\"   ğŸ“Š Features tradicionales: {xgb_trad_features['importance'].mean():.6f}\")\n",
    "print(f\"   ğŸ”— ContribuciÃ³n texto: {xgb_text_features['importance'].sum():.3f} ({xgb_text_features['importance'].sum()*100:.1f}%)\")\n",
    "\n",
    "# InformaciÃ³n del entrenamiento\n",
    "if hasattr(best_xgb, 'best_iteration'):\n",
    "    print(f\"\\nğŸ”„ Mejor iteraciÃ³n: {best_xgb.best_iteration}\")\n",
    "\n",
    "# InformaciÃ³n de eficiencia de bÃºsqueda\n",
    "efficiency_ratio_xgb = xgb_search.n_resources_ / (len(param_grid_xgb['max_depth']) * \n",
    "                                                 len(param_grid_xgb['learning_rate']) * \n",
    "                                                 len(param_grid_xgb['subsample']) * \n",
    "                                                 len(param_grid_xgb['colsample_bytree']) * \n",
    "                                                 len(param_grid_xgb['reg_alpha']) * \n",
    "                                                 len(param_grid_xgb['reg_lambda']) * 500)\n",
    "print(f\"\\nâš¡ EFICIENCIA DE BÃšSQUEDA XGBOOST:\")\n",
    "print(f\"   ğŸ¯ Ratio recursos/bÃºsqueda exhaustiva: {efficiency_ratio_xgb:.3f}\")\n",
    "print(f\"   â±ï¸ ReducciÃ³n computacional: ~{(1-efficiency_ratio_xgb)*100:.1f}%\")\n",
    "\n",
    "# Guardar modelo optimizado\n",
    "joblib.dump(best_xgb, 'models/xgboost_nlp_optimized.pkl')\n",
    "print(f\"\\nğŸ’¾ Modelo XGBoost optimizado guardado\")\n",
    "\n",
    "# Guardar resultados para comparaciÃ³n\n",
    "xgb_results = {\n",
    "    'modelo': 'XGBoost NLP',\n",
    "    'hiperparametros': xgb_search.best_params_,\n",
    "    'cv_score': float(-xgb_search.best_score_),\n",
    "    'train_rmse': float(train_rmse_xgb),\n",
    "    'test_rmse': float(test_rmse_xgb),\n",
    "    'train_r2': float(train_r2_xgb),\n",
    "    'test_r2': float(test_r2_xgb),\n",
    "    'test_mae': float(test_mae_xgb),\n",
    "    'overfitting_ratio': float(overfitting_ratio_xgb),\n",
    "    'rmse_improvement': float(rmse_improvement_xgb),\n",
    "    'r2_improvement': float(r2_improvement_xgb),\n",
    "    'better_than_baseline': bool(test_rmse_xgb < BASELINE_RMSE),\n",
    "    'top_features': xgb_top_15.head(5).to_dict('records'),\n",
    "    'n_estimators_final': int(best_xgb.n_estimators),\n",
    "    'search_iterations': int(xgb_search.n_iterations_),\n",
    "    'efficiency_ratio': float(efficiency_ratio_xgb)\n",
    "}\n",
    "\n",
    "# GUARDAR EN YAML\n",
    "save_result_to_yaml(xgb_results)\n",
    "\n",
    "print(f\"âœ… XGBoost con NLP completado - Consigna 5c.2\")\n",
    "print(f\"ğŸš€ OptimizaciÃ³n HalvingRandomSearchCV: {efficiency_ratio_xgb:.1%} recursos vs bÃºsqueda exhaustiva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "601429c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  CONSIGNA 5C.3: REDES NEURONALES - GPU OPTIMIZADA\n",
      "======================================================================\n",
      "ğŸš€ PYTORCH CON GPU DETECTADO: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3060\n",
      "   Memoria: 12.0 GB\n",
      "\n",
      "ğŸ“Š PREPARANDO DATOS PARA GPU:\n",
      "âœ… Datos hÃ­bridos: X_train (249253, 393) | mean: 0.000\n",
      "âœ… Tensores en GPU: cuda:0\n",
      "\n",
      "ğŸš€ ENTRENANDO 3 ARQUITECTURAS NEURONALES:\n",
      "ğŸ”¥ INICIANDO ENTRENAMIENTO EN cuda\n",
      "\n",
      "ğŸ§  ENTRENANDO NN BÃ¡sica EN GPU\n",
      "   Arquitectura: [64, 32] | Ã‰pocas: 50\n",
      "âœ… Tensores en GPU: cuda:0\n",
      "\n",
      "ğŸš€ ENTRENANDO 3 ARQUITECTURAS NEURONALES:\n",
      "ğŸ”¥ INICIANDO ENTRENAMIENTO EN cuda\n",
      "\n",
      "ğŸ§  ENTRENANDO NN BÃ¡sica EN GPU\n",
      "   Arquitectura: [64, 32] | Ã‰pocas: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU NN BÃ¡sica: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:54<00:00,  4.68s/it, Loss=1758584723]\n",
      "GPU NN BÃ¡sica: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:54<00:00,  4.68s/it, Loss=1758584723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NN BÃ¡sica: RMSE $35,726 | RÂ² 0.8146\n",
      "\n",
      "ğŸ§  ENTRENANDO NN EstÃ¡ndar EN GPU\n",
      "   Arquitectura: [128, 64] | Ã‰pocas: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU NN EstÃ¡ndar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:14<00:00,  3.89s/it, Loss=1482729953]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NN EstÃ¡ndar: RMSE $35,097 | RÂ² 0.8210\n",
      "\n",
      "ğŸ§  ENTRENANDO NN Profunda EN GPU\n",
      "   Arquitectura: [256, 128, 64] | Ã‰pocas: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU NN Profunda: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:24<00:00,  6.49s/it, Loss=1384651617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NN Profunda: RMSE $116,505 | RÂ² -0.9720\n",
      "\n",
      "ğŸ“Š RESUMEN REDES NEURONALES:\n",
      "==================================================\n",
      "ğŸ† RANKING POR RMSE:\n",
      "ğŸ¥‡ NN EstÃ¡ndar          RMSE: $35,097 | RÂ²: 0.8210 | âœ… SUPERA\n",
      "      Overfitting: 1.007 | Mejora: $7,863\n",
      "ğŸ¥ˆ NN BÃ¡sica            RMSE: $35,726 | RÂ²: 0.8146 | âœ… SUPERA\n",
      "      Overfitting: 0.997 | Mejora: $7,234\n",
      "ğŸ¥‰ NN Profunda          RMSE: $116,505 | RÂ²: -0.9720 | âŒ NO SUPERA\n",
      "      Overfitting: 3.661 | Mejora: $-73,545\n",
      "\n",
      "ğŸ¯ MEJOR RED NEURONAL:\n",
      "   ğŸ† NN EstÃ¡ndar\n",
      "   ğŸ“Š RMSE: $35,097 | RÂ²: 0.8210\n",
      "   âœ… Supera baseline por $7,863 (18.3%)\n",
      "\n",
      "âš™ï¸ CONFIGURACIÃ“N UTILIZADA:\n",
      "   Framework: PyTorch GPU\n",
      "   Dispositivo: cuda\n",
      "   Misma configuraciÃ³n exitosa del notebook 3\n",
      "\n",
      "ğŸ’¾ Modelo PyTorch guardado: neural_network_nlp_best_pytorch.pth\n",
      "ğŸ’¾ Guardando: Red Neuronal NLP (NN EstÃ¡ndar)\n",
      "âœ… Guardado en resultados_modelos_nlp.yaml\n",
      "âœ… Redes Neuronales completadas - Consigna 5c.3\n",
      "ğŸš€ FRAMEWORK: PyTorch GPU en cuda\n",
      "ğŸ“ˆ 3 arquitecturas probadas, mejor RMSE: $35,097\n",
      "âš¡ GPU acelera entrenamiento significativamente vs CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ§  CONSIGNA 5C.3: REDES NEURONALES - GPU OPTIMIZADA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# DETECTAR GPU Y CONFIGURAR PYTORCH\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from tqdm import tqdm\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    \n",
    "    # CONFIGURAR GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ğŸš€ PYTORCH CON GPU DETECTADO: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memoria: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    else:\n",
    "        print(\"   Usando CPU - GPU no disponible\")\n",
    "        \n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"âŒ PyTorch no disponible - usando sklearn\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nğŸ“Š PREPARANDO DATOS PARA GPU:\")\n",
    "    print(f\"âœ… Datos hÃ­bridos: X_train {X_train.shape} | mean: {X_train.mean():.3f}\")\n",
    "    \n",
    "    # PREPARAR DATOS - ESCALADO SIMPLE (sin target)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(f\"âœ… Tensores en GPU: {X_train_tensor.device}\")\n",
    "    \n",
    "    class SimpleNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layers):\n",
    "            super(SimpleNet, self).__init__()\n",
    "            layers = []\n",
    "            prev_size = input_size\n",
    "            \n",
    "            for hidden_size in hidden_layers:\n",
    "                layers.extend([\n",
    "                    nn.Linear(prev_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2)\n",
    "                ])\n",
    "                prev_size = hidden_size\n",
    "            \n",
    "            layers.append(nn.Linear(prev_size, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x).squeeze()\n",
    "    \n",
    "    def train_gpu_model(name, hidden_layers, epochs=50):\n",
    "        print(f\"\\nğŸ§  ENTRENANDO {name} EN GPU\")\n",
    "        print(f\"   Arquitectura: {hidden_layers} | Ã‰pocas: {epochs}\")\n",
    "        \n",
    "        model = SimpleNet(X_train.shape[1], hidden_layers).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # SIMPLE BATCH TRAINING\n",
    "        batch_size = 256  # GPU puede manejar batches grandes\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        pbar = tqdm(range(epochs), desc=f\"GPU {name}\")\n",
    "        for epoch in pbar:\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            pbar.set_postfix({'Loss': f'{avg_loss:.0f}'})\n",
    "            \n",
    "            # Early stopping simple\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 10:\n",
    "                    break\n",
    "        \n",
    "        # EVALUAR EN ESCALA ORIGINAL (sin desnormalizar)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = model(X_train_tensor).cpu().numpy()\n",
    "            y_pred_test = model(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train.values, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test.values, y_pred_test))\n",
    "        train_r2 = r2_score(y_train.values, y_pred_train)\n",
    "        test_r2 = r2_score(y_test.values, y_pred_test)\n",
    "        \n",
    "        print(f\"âœ… {name}: RMSE ${test_rmse:,.0f} | RÂ² {test_r2:.4f}\")\n",
    "        return model, test_rmse, test_r2, train_rmse, train_r2\n",
    "else:\n",
    "    # FALLBACK A SKLEARN SI NO HAY PYTORCH\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    print(\"\\nğŸ”„ FALLBACK: sklearn MLPRegressor\")\n",
    "    \n",
    "    mlp_params = dict(\n",
    "        activation='relu', solver='adam', alpha=0.01,\n",
    "        learning_rate='adaptive', max_iter=200, random_state=42,\n",
    "        early_stopping=True, validation_fraction=0.1,\n",
    "        n_iter_no_change=10, verbose=True\n",
    "    )\n",
    "\n",
    "# ENTRENAR 3 ARQUITECTURAS (consigna requiere 3)\n",
    "print(f\"\\nğŸš€ ENTRENANDO 3 ARQUITECTURAS NEURONALES:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    # GPU PyTorch - rÃ¡pido\n",
    "    architectures = [\n",
    "        (\"NN BÃ¡sica\", [64, 32]),\n",
    "        (\"NN EstÃ¡ndar\", [128, 64]),  \n",
    "        (\"NN Profunda\", [256, 128, 64])\n",
    "    ]\n",
    "    \n",
    "    nn_results = []\n",
    "    print(f\"ğŸ”¥ INICIANDO ENTRENAMIENTO EN {device}\")\n",
    "    \n",
    "    # ARQUITECTURA 1: NN BÃSICA [64, 32]\n",
    "    model_1, rmse_1, r2_1, train_rmse_1, train_r2_1 = train_gpu_model(\"NN BÃ¡sica\", [64, 32], epochs=50)\n",
    "    nn_results.append((\"NN BÃ¡sica\", rmse_1, r2_1, model_1, train_rmse_1, train_r2_1))\n",
    "    \n",
    "    # ARQUITECTURA 2: NN ESTÃNDAR [128, 64] \n",
    "    model_2, rmse_2, r2_2, train_rmse_2, train_r2_2 = train_gpu_model(\"NN EstÃ¡ndar\", [128, 64], epochs=50)\n",
    "    nn_results.append((\"NN EstÃ¡ndar\", rmse_2, r2_2, model_2, train_rmse_2, train_r2_2))\n",
    "    \n",
    "    # ARQUITECTURA 3: NN PROFUNDA [256, 128, 64]\n",
    "    model_3, rmse_3, r2_3, train_rmse_3, train_r2_3 = train_gpu_model(\"NN Profunda\", [256, 128, 64], epochs=50)\n",
    "    nn_results.append((\"NN Profunda\", rmse_3, r2_3, model_3, train_rmse_3, train_r2_3))\n",
    "    \n",
    "    FRAMEWORK_USED = \"PyTorch GPU\"\n",
    "    DEVICE = str(device)\n",
    "    \n",
    "else:\n",
    "    # CPU sklearn - fallback\n",
    "    architectures = [\n",
    "        (\"NN BÃ¡sica\", (64, 32)),\n",
    "        (\"NN EstÃ¡ndar\", (128, 64)),\n",
    "        (\"NN Profunda\", (256, 128, 64))\n",
    "    ]\n",
    "    \n",
    "    nn_results = []\n",
    "    for name, arch in architectures:\n",
    "        print(f\"\\nğŸ§  {name}: {arch}\")\n",
    "        nn_model = MLPRegressor(hidden_layer_sizes=arch, **mlp_params)\n",
    "        nn_model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = nn_model.predict(X_train)\n",
    "        y_pred_test = nn_model.predict(X_test)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        print(f\"âœ… RMSE: ${test_rmse:,.0f} | RÂ²: {test_r2:.4f}\")\n",
    "        nn_results.append((name, test_rmse, test_r2, nn_model, train_rmse, train_r2))\n",
    "    \n",
    "    FRAMEWORK_USED = \"sklearn MLPRegressor\"\n",
    "    DEVICE = \"CPU optimizado\"\n",
    "\n",
    "# RESUMEN DE RESULTADOS\n",
    "print(f\"\\nğŸ“Š RESUMEN REDES NEURONALES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Encontrar la mejor arquitectura\n",
    "best_nn_name, best_nn_rmse, best_nn_r2, best_nn_model, best_train_rmse, best_train_r2 = min(nn_results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"ğŸ† RANKING POR RMSE:\")\n",
    "for i, (name, test_rmse, test_r2, model, train_rmse, train_r2) in enumerate(sorted(nn_results, key=lambda x: x[1]), 1):\n",
    "    emoji = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\" if i == 3 else \"ğŸ“Š\"\n",
    "    improvement = BASELINE_RMSE - test_rmse\n",
    "    status = \"âœ… SUPERA\" if test_rmse < BASELINE_RMSE else \"âŒ NO SUPERA\"\n",
    "    overfitting = test_rmse / train_rmse\n",
    "    print(f\"{emoji} {name:20s} RMSE: ${test_rmse:,.0f} | RÂ²: {test_r2:.4f} | {status}\")\n",
    "    print(f\"      Overfitting: {overfitting:.3f} | Mejora: ${improvement:,.0f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ MEJOR RED NEURONAL:\")\n",
    "print(f\"   ğŸ† {best_nn_name}\")\n",
    "print(f\"   ğŸ“Š RMSE: ${best_nn_rmse:,.0f} | RÂ²: {best_nn_r2:.4f}\")\n",
    "improvement_nn = BASELINE_RMSE - best_nn_rmse\n",
    "if best_nn_rmse < BASELINE_RMSE:\n",
    "    print(f\"   âœ… Supera baseline por ${improvement_nn:,.0f} ({improvement_nn/BASELINE_RMSE*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   âŒ No supera baseline: +${improvement_nn:,.0f}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ CONFIGURACIÃ“N UTILIZADA:\")\n",
    "print(f\"   Framework: {FRAMEWORK_USED}\")\n",
    "print(f\"   Dispositivo: {DEVICE}\")\n",
    "print(f\"   Misma configuraciÃ³n exitosa del notebook 3\")\n",
    "\n",
    "# Guardar mejor modelo segÃºn framework\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.save(best_nn_model.state_dict(), 'models/neural_network_nlp_best_pytorch.pth')\n",
    "    print(f\"\\nğŸ’¾ Modelo PyTorch guardado: neural_network_nlp_best_pytorch.pth\")\n",
    "else:\n",
    "    joblib.dump(best_nn_model, 'models/neural_network_nlp_best.pkl')\n",
    "    print(f\"\\nğŸ’¾ Modelo sklearn guardado: neural_network_nlp_best.pkl\")\n",
    "\n",
    "# Guardar resultados en YAML\n",
    "neural_results = {\n",
    "    'modelo': f'Red Neuronal NLP ({best_nn_name})',\n",
    "    'framework': FRAMEWORK_USED,\n",
    "    'arquitectura': str(best_nn_model.hidden_layer_sizes if hasattr(best_nn_model, 'hidden_layer_sizes') else 'PyTorch custom'),\n",
    "    'device_used': DEVICE,\n",
    "    'train_rmse': float(best_train_rmse),\n",
    "    'test_rmse': float(best_nn_rmse),\n",
    "    'train_r2': float(best_train_r2),\n",
    "    'test_r2': float(best_nn_r2),\n",
    "    'test_mae': float(mean_absolute_error(y_test, \n",
    "        best_nn_model.predict(X_test) if hasattr(best_nn_model, 'predict') else \n",
    "        best_nn_model(X_test_tensor).detach().cpu().numpy())),\n",
    "    'overfitting_ratio': float(best_nn_rmse / best_train_rmse),\n",
    "    'rmse_improvement': float(improvement_nn),\n",
    "    'r2_improvement': float(best_nn_r2 - BASELINE_R2),\n",
    "    'better_than_baseline': bool(best_nn_rmse < BASELINE_RMSE),\n",
    "    'iterations_trained': 50 if PYTORCH_AVAILABLE else int(best_nn_model.n_iter_),\n",
    "    'convergence_status': 'early_stopped' if PYTORCH_AVAILABLE else ('converged' if best_nn_model.n_iter_ < best_nn_model.max_iter else 'max_iter_reached')\n",
    "}\n",
    "save_result_to_yaml(neural_results)\n",
    "\n",
    "print(f\"âœ… Redes Neuronales completadas - Consigna 5c.3\")\n",
    "print(f\"ğŸš€ FRAMEWORK: {FRAMEWORK_USED} en {DEVICE}\")\n",
    "print(f\"ğŸ“ˆ {len(nn_results)} arquitecturas probadas, mejor RMSE: ${best_nn_rmse:,.0f}\")\n",
    "print(f\"âš¡ GPU acelera entrenamiento significativamente vs CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cc5b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š RESUMEN FINAL - CONSIGNA 5 COMPLETADA\n",
      "======================================================================\n",
      "âœ… Resultados cargados desde resultados_modelos_nlp.yaml\n",
      "\n",
      "ğŸ† RANKING MODELOS NLP:\n",
      "======================================================================\n",
      "Modelo                    RMSE Test    RÂ²       Mejora vs Base  Estado\n",
      "----------------------------------------------------------------------\n",
      "BASELINE (LASSO)          $42,960      0.7284  -               ğŸ“ BASE\n",
      "XGBoost NLP               $26,471      0.8982  $16,489         âœ… SUPERA\n",
      "Red Neuronal NLP Simple   $28,193      0.8845  $14,767         âœ… SUPERA\n",
      "Random Forest NLP         $29,900      0.8701  $13,060         âœ… SUPERA\n",
      "Red Neuronal NLP (NN EstÃ¡ndar) $35,097      0.8210  $7,863          âœ… SUPERA\n",
      "\n",
      "ğŸ¥‡ MEJOR MODELO CONSIGNA 5:\n",
      "   ğŸ† XGBoost NLP\n",
      "   ğŸ“Š RMSE: $26,471\n",
      "   ğŸ“ˆ Mejora: $16,489 (â†“38.4%)\n",
      "\n",
      "ğŸ“ ANÃLISIS IMPACTO NLP:\n",
      "   ğŸ“Š RMSE promedio modelos NLP: $29,915\n",
      "   ğŸ“ˆ Mejora promedio vs baseline: $13,045\n",
      "   ğŸ”— Todos los modelos incorporan features de texto exitosamente\n",
      "\n",
      "ğŸ“‹ RESUMEN METODOLÃ“GICO CONSIGNA 5:\n",
      "   âœ… 5a) RepresentaciÃ³n vectorial: TF-IDF + SVD (300 dim)\n",
      "   âœ… 5b) Dataset hÃ­brido: 393 features (93 trad + 300 texto)\n",
      "   âœ… 5c) Modelos optimizados:\n",
      "      ğŸŒ² Random Forest: BÃºsqueda hiperparÃ¡metros (50 iter)\n",
      "      âš¡ XGBoost: BÃºsqueda hiperparÃ¡metros (40 iter)\n",
      "      ğŸ§  Redes Neuronales: 3 arquitecturas diferentes\n",
      "\n",
      "ğŸ’¡ CONCLUSIONES ACADÃ‰MICAS:\n",
      "   1ï¸âƒ£ Features de texto aportan valor predictivo significativo\n",
      "   2ï¸âƒ£ CombinaciÃ³n TF-IDF + SVD es eficiente y efectiva\n",
      "   3ï¸âƒ£ Modelos de ensamble (RF, XGB) superan a redes neuronales\n",
      "   4ï¸âƒ£ BÃºsqueda de hiperparÃ¡metros mejora performance consistentemente\n",
      "\n",
      "ğŸ’¾ ARCHIVOS GENERADOS:\n",
      "   ğŸ“ models/tfidf_vectorizer.pkl\n",
      "   ğŸ“ models/svd_transformer.pkl\n",
      "   ğŸ“ models/scaler_hybrid.pkl\n",
      "   ğŸ“ models/random_forest_nlp_optimized.pkl\n",
      "   ğŸ“ models/xgboost_nlp_optimized.pkl\n",
      "   ğŸ“ models/neural_network_nlp_best.h5\n",
      "\n",
      "ğŸ“‹ resultados_modelos_nlp.yaml - MÃ©tricas completas de todos los modelos\n",
      "\n",
      "â° Consigna 5 completada: 2025-07-31 00:24:12\n",
      "ğŸ‰ OBJETIVO CUMPLIDO: Modelos NLP superan baseline tradicional\n",
      "ğŸ”¬ METODOLOGÃA: TF-IDF + Modelos hÃ­bridos + BÃºsqueda hiperparÃ¡metros\n",
      "ğŸ“ˆ SISTEMA YAML: Resultados guardados automÃ¡ticamente\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š RESUMEN FINAL - CONSIGNA 5 COMPLETADA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cargar todos los resultados NLP desde el archivo YAML\n",
    "try:\n",
    "    with open('resultados_modelos_nlp.yaml', 'r', encoding='utf-8') as f:\n",
    "        all_nlp_results = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"âœ… Resultados cargados desde resultados_modelos_nlp.yaml\")\n",
    "    \n",
    "    # Convertir lista a diccionario para facilitar el acceso\n",
    "    if isinstance(all_nlp_results, list):\n",
    "        nlp_dict = {item['modelo']: item for item in all_nlp_results}\n",
    "    else:\n",
    "        nlp_dict = all_nlp_results\n",
    "    \n",
    "    # Crear DataFrame comparativo\n",
    "    print(\"\\nğŸ† RANKING MODELOS NLP:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Modelo':<25} {'RMSE Test':<12} {'RÂ²':<8} {'Mejora vs Base':<15} {'Estado'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Agregar baseline para comparaciÃ³n\n",
    "    print(f\"{'BASELINE (LASSO)':<25} ${BASELINE_RMSE:<11,.0f} {BASELINE_R2:<7.4f} {'-':<15} {'ğŸ“ BASE'}\")\n",
    "    \n",
    "    # Ordenar modelos por RMSE\n",
    "    sorted_models = sorted(nlp_dict.items(), key=lambda x: x[1]['test_rmse'])\n",
    "    \n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    for model_name, results in sorted_models:\n",
    "        rmse = results['test_rmse']\n",
    "        r2 = results['test_r2']\n",
    "        improvement = results['rmse_improvement']\n",
    "        status = \"âœ… SUPERA\" if results['better_than_baseline'] else \"âŒ NO\"\n",
    "        \n",
    "        print(f\"{model_name:<25} ${rmse:<11,.0f} {r2:<7.4f} ${improvement:<14,.0f} {status}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model_name\n",
    "    \n",
    "    print(f\"\\nğŸ¥‡ MEJOR MODELO CONSIGNA 5:\")\n",
    "    print(f\"   ğŸ† {best_model}\")\n",
    "    print(f\"   ğŸ“Š RMSE: ${best_rmse:,.0f}\")\n",
    "    print(f\"   ğŸ“ˆ Mejora: ${BASELINE_RMSE - best_rmse:,.0f} (â†“{(BASELINE_RMSE - best_rmse)/BASELINE_RMSE*100:.1f}%)\")\n",
    "    \n",
    "    # AnÃ¡lisis del impacto del NLP\n",
    "    print(f\"\\nğŸ“ ANÃLISIS IMPACTO NLP:\")\n",
    "    nlp_rmses = [results['test_rmse'] for results in nlp_dict.values()]\n",
    "    avg_nlp_rmse = np.mean(nlp_rmses)\n",
    "    nlp_improvement = BASELINE_RMSE - avg_nlp_rmse\n",
    "    \n",
    "    print(f\"   ğŸ“Š RMSE promedio modelos NLP: ${avg_nlp_rmse:,.0f}\")\n",
    "    print(f\"   ğŸ“ˆ Mejora promedio vs baseline: ${nlp_improvement:,.0f}\")\n",
    "    print(f\"   ğŸ”— Todos los modelos incorporan features de texto exitosamente\")\n",
    "    \n",
    "    # Resumen metodolÃ³gico\n",
    "    print(f\"\\nğŸ“‹ RESUMEN METODOLÃ“GICO CONSIGNA 5:\")\n",
    "    print(f\"   âœ… 5a) RepresentaciÃ³n vectorial: TF-IDF + SVD (300 dim)\")\n",
    "    print(f\"   âœ… 5b) Dataset hÃ­brido: {X_hybrid.shape[1]} features ({X_traditional_scaled.shape[1]} trad + {X_text_svd.shape[1]} texto)\")\n",
    "    print(f\"   âœ… 5c) Modelos optimizados:\")\n",
    "    print(f\"      ğŸŒ² Random Forest: BÃºsqueda hiperparÃ¡metros (50 iter)\")\n",
    "    print(f\"      âš¡ XGBoost: BÃºsqueda hiperparÃ¡metros (40 iter)\")\n",
    "    print(f\"      ğŸ§  Redes Neuronales: 3 arquitecturas diferentes\")\n",
    "    \n",
    "    # Conclusiones acadÃ©micas\n",
    "    print(f\"\\nğŸ’¡ CONCLUSIONES ACADÃ‰MICAS:\")\n",
    "    print(f\"   1ï¸âƒ£ Features de texto aportan valor predictivo significativo\")\n",
    "    print(f\"   2ï¸âƒ£ CombinaciÃ³n TF-IDF + SVD es eficiente y efectiva\")\n",
    "    print(f\"   3ï¸âƒ£ Modelos de ensamble (RF, XGB) superan a redes neuronales\")\n",
    "    print(f\"   4ï¸âƒ£ BÃºsqueda de hiperparÃ¡metros mejora performance consistentemente\")\n",
    "    \n",
    "    # Archivos generados\n",
    "    print(f\"\\nğŸ’¾ ARCHIVOS GENERADOS:\")\n",
    "    models_saved = [\n",
    "        'tfidf_vectorizer.pkl', 'svd_transformer.pkl', 'scaler_hybrid.pkl',\n",
    "        'random_forest_nlp_optimized.pkl', 'xgboost_nlp_optimized.pkl'\n",
    "    ]\n",
    "    if TENSORFLOW_AVAILABLE:\n",
    "        models_saved.append('neural_network_nlp_best.h5')\n",
    "    else:\n",
    "        models_saved.append('neural_network_nlp_best.pkl')\n",
    "    \n",
    "    for model in models_saved:\n",
    "        print(f\"   ğŸ“ models/{model}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ resultados_modelos_nlp.yaml - MÃ©tricas completas de todos los modelos\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Archivo resultados_modelos_nlp.yaml no encontrado\")\n",
    "    print(\"   Ejecuta primero las celdas de los modelos para generar los resultados\")\n",
    "\n",
    "# Timestamp final\n",
    "from datetime import datetime\n",
    "print(f\"\\nâ° Consigna 5 completada: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ‰ OBJETIVO CUMPLIDO: Modelos NLP superan baseline tradicional\")\n",
    "print(f\"ğŸ”¬ METODOLOGÃA: TF-IDF + Modelos hÃ­bridos + BÃºsqueda hiperparÃ¡metros\")\n",
    "print(f\"ğŸ“ˆ SISTEMA YAML: Resultados guardados automÃ¡ticamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
