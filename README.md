# üè† Trabajo Final - Inteligencia de Negocios 2025

**Maestr√≠a en Econom√≠a Aplicada - Facultad de Ciencias Econ√≥micas - UBA**

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un sistema completo de **predicci√≥n de precios de inmuebles** utilizando t√©cnicas de Machine Learning y Procesamiento de Lenguaje Natural (NLP). El objetivo es predecir el precio de propiedades bas√°ndose en caracter√≠sticas num√©ricas, categ√≥ricas y descripciones textuales.

## üóÇÔ∏è Estructura del Proyecto

### üìÅ Archivos de Datos
- `train_bi_2025.csv` - Dataset de entrenamiento original (~400k registros)
- `test_bi_2025.csv` - Dataset de prueba para evaluaci√≥n final
- `train_bi_2025_filtered.csv` - Dataset filtrado (generado autom√°ticamente)

### üìì Notebooks del An√°lisis

#### 1. **01_analisis_exploratorio.ipynb** üîç
**Duraci√≥n estimada:** 10-15 minutos
- ‚úÖ Carga y exploraci√≥n inicial del dataset
- ‚úÖ An√°lisis de tipos de datos y valores faltantes
- ‚úÖ Matriz de correlaciones y estad√≠sticas descriptivas
- ‚úÖ An√°lisis por tipo de propiedad con visualizaciones
- ‚úÖ Detecci√≥n y eliminaci√≥n de outliers (m√©todo IQR)
- ‚úÖ Optimizaci√≥n de variables temporales para ML
- ‚úÖ Generaci√≥n de dataset filtrado y optimizado

#### 2. **02_modelos_tradicionales.ipynb** üìà
**Duraci√≥n estimada:** 5-10 minutos
- ‚úÖ Preparaci√≥n de datos con encoding autom√°tico
- ‚úÖ **Regresi√≥n Lineal:** An√°lisis de coeficientes y significancia
- ‚úÖ **LASSO:** Optimizaci√≥n con validaci√≥n cruzada, selecci√≥n de variables
- ‚úÖ Interpretaci√≥n econ√≥mica de coeficientes
- ‚úÖ An√°lisis de significatividad estad√≠stica vs pr√°ctica
- ‚úÖ Comparaci√≥n final con visualizaciones

#### 3. **03_modelos_ml.ipynb** ü§ñ
**Duraci√≥n estimada:** 15-25 minutos
- ‚úÖ **Random Forest:** Optimizaci√≥n con RandomizedSearchCV
- ‚úÖ **XGBoost:** Configuraci√≥n optimizada con verbose para seguimiento
- ‚úÖ **Redes Neuronales (MLP):** Arquitecturas optimizadas
- ‚úÖ **An√°lisis de overfitting:** Clasificaci√≥n de robustez
- ‚úÖ Importancia de variables y visualizaciones
- ‚úÖ Comparaci√≥n con m√©tricas de generalizaci√≥n

#### 4. **04_modelos_nlp.ipynb** üìù
**Duraci√≥n estimada:** 15-25 minutos
- ‚úÖ **Procesamiento de texto:** Limpieza con stopwords en espa√±ol
- ‚úÖ **TF-IDF + SVD:** Reducci√≥n dimensional (5000‚Üí300 componentes)
- ‚úÖ **Modelos h√≠bridos:** Combinaci√≥n de features tradicionales + texto
- ‚úÖ **Random Forest NLP:** Optimizaci√≥n con HalvingRandomSearchCV
- ‚úÖ **XGBoost NLP:** Optimizaci√≥n avanzada con GPU
- ‚úÖ **Redes Neuronales NLP:** 3 arquitecturas diferentes (PyTorch GPU)
- ‚úÖ **An√°lisis de t√©rminos:** Palabras m√°s predictivas del precio
- ‚úÖ **Comparaci√≥n integral:** Modelos con y sin texto

#### 5. **06_evaluacion_final_performance.ipynb** üìä
**Duraci√≥n estimada:** 5-10 minutos
- ‚úÖ **Consigna 6:** Evaluaci√≥n de 6 modelos optimizados sobre test
- ‚úÖ **An√°lisis comparativo:** Modelos con vs sin descripciones
- ‚úÖ **M√©tricas RMSE y MAE:** Reportes detallados
- ‚úÖ **Conclusiones:** Informaci√≥n relevante en descripciones de texto
- ‚úÖ **Visualizaciones:** Gr√°ficos comparativos de performance

## üöÄ C√≥mo Ejecutar el Proyecto

### Opci√≥n 1: Ejecuci√≥n Secuencial (Recomendada)
```bash
1. Ejecutar: 01_analisis_exploratorio.ipynb
2. Ejecutar: 02_modelos_tradicionales.ipynb
3. Ejecutar: 03_modelos_ml.ipynb
4. Ejecutar: 04_modelos_nlp.ipynb
5. Ejecutar: 06_evaluacion_final_performance.ipynb
```

### Opci√≥n 2: Ejecuci√≥n Independiente
Cada notebook es **auto-contenida** y puede ejecutarse independientemente. Cada una carga y prepara sus propios datos.

## üì¶ Dependencias

### Librer√≠as Requeridas:
```python
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
xgboost>=1.5.0
tqdm>=4.62.0
```

### Instalaci√≥n:
```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost tqdm
```

## üìä Resultados Obtenidos

### üéØ Mejores Modelos por Categor√≠a:

| Categor√≠a | Mejor Modelo | RMSE | R¬≤ | MAE | Features | Overfitting |
|-----------|-------------|------|----|----|---------|-------------|
| **Tradicional** | Regresi√≥n Lineal | $42,960 | 0.7285 | $30,922 | 93 | 0.991 ‚úÖ |
| **Machine Learning** | XGBoost Optimizado | $34,623 | 0.8236 | $23,967 | 93 | 1.076 ‚úÖ |
| **NLP H√≠brido** | **XGBoost NLP** | **$26,471** | **0.8982** | **$17,284** | **393** | **1.487 ‚ö†Ô∏è** |

### üèÜ **Modelo Ganador: XGBoost NLP**
- **RMSE:** $26,471 (mejor rendimiento absoluto)
- **R¬≤:** 0.8982 (explica 89.8% de la variabilidad)
- **MAE:** $17,284 (error absoluto medio m√°s bajo)
- **Overfitting Ratio:** 1.487 (overfitting moderado pero aceptable)
- **Features:** 393 (93 tradicionales + 300 texto TF-IDF/SVD)
- **Mejora vs XGBoost tradicional:** 23.5% en RMSE

### üìà **Ranking Completo de Modelos (Top 8)**:
1. **XGBoost NLP** - $26,471 RMSE, R¬≤ 0.8982, MAE $17,284 ‚ö†Ô∏è
2. **Red Neuronal NLP Simple** - $28,193 RMSE, R¬≤ 0.8845, MAE $19,013 ‚úÖ
3. **Random Forest NLP** - $29,900 RMSE, R¬≤ 0.8701, MAE $19,455 ‚ùå
4. **XGBoost Optimizado** - $34,623 RMSE, R¬≤ 0.8236, MAE $23,967 ‚úÖ
5. **Red Neuronal NLP Est√°ndar** - $35,097 RMSE, R¬≤ 0.8210, MAE $24,640 ‚úÖ
6. **Red Neuronal Optimizada** - $40,321 RMSE, R¬≤ 0.7608, MAE $28,231 ‚úÖ
7. **Random Forest Optimizado** - $40,837 RMSE, R¬≤ 0.7547, MAE $28,963 ‚úÖ
8. **Regresi√≥n Lineal** - $42,960 RMSE, R¬≤ 0.7285, MAE $30,922 ‚úÖ

## üîç Caracter√≠sticas del Dataset

### Variables Principales:
- **Num√©ricas:** rooms, bathrooms, surface_total, surface_covered
- **Temporales:** created_year, created_month, created_quarter, created_weekday
- **Categ√≥ricas:** l2, l3, prop_type  
- **Texto:** description (procesada con TF-IDF + SVD)
- **Target:** price

### Procesamiento:
- **Dataset original:** 400k+ registros
- **Dataset filtrado:** 311,660 registros (eliminaci√≥n de outliers IQR)
- **Features tradicionales:** 93 (post one-hot encoding)
- **Features de texto:** 300 (post SVD de 5,000 t√©rminos TF-IDF)
- **Features h√≠bridas:** 393 (tradicionales + texto)

## ‚ö° Optimizaciones Implementadas

### Rendimiento:
- ‚úÖ **HalvingRandomSearchCV** para b√∫squeda eficiente de hiperpar√°metros
- ‚úÖ **PyTorch GPU** para redes neuronales aceleradas
- ‚úÖ **SVD** para reducci√≥n dimensional del texto (5000‚Üí300)
- ‚úÖ **Early stopping** en redes neuronales
- ‚úÖ **Memory management** para datasets grandes

### Robustez:
- ‚úÖ **An√°lisis de overfitting:** Clasificaci√≥n autom√°tica por ratios
- ‚úÖ **Validaci√≥n cruzada** en optimizaci√≥n de hiperpar√°metros
- ‚úÖ **M√©tricas m√∫ltiples:** RMSE, R¬≤, MAE, ratios de generalizaci√≥n
- ‚úÖ **Comparaci√≥n integral:** 8+ modelos evaluados
- ‚úÖ **Evaluaci√≥n separada:** Train, validaci√≥n y test independientes

## üìà Insights Clave

### Variables M√°s Importantes (Modelos Tradicionales):
1. **surface_total** - Superficie total (coef: +$49,878 por m¬≤)
2. **bathrooms** - N√∫mero de ba√±os (coef: +$13,806 por ba√±o)
3. **rooms** - N√∫mero de habitaciones (coef: +$5,618 por habitaci√≥n)
4. **l3_Puerto Madero** - Ubicaci√≥n premium (coef: +$178,390)
5. **l3_Villa Soldati** - Ubicaci√≥n desfavorable (coef: -$86,924)

### Variables M√°s Importantes (Modelos NLP):
1. **trad_surface_covered** - Superficie cubierta (34.1% importancia RF)
2. **trad_surface_total** - Superficie total (20.3% importancia RF)
3. **trad_l3_Puerto Madero** - Ubicaci√≥n premium (6.1% importancia XGB)
4. **texto_dim_12** - Dimensi√≥n textual 12 (4.8% importancia RF)
5. **trad_bathrooms** - N√∫mero de ba√±os (6.3% importancia RF)

### Impacto del NLP:
- ‚úÖ **XGBoost:** 23.5% mejora en RMSE con texto
- ‚úÖ **Red Neuronal Simple:** 30.1% mejora en RMSE con texto  
- ‚úÖ **Random Forest:** 20.4% mejora en RMSE con texto
- ‚úÖ **Contribuci√≥n promedio:** ~25% mejora en performance
- ‚úÖ **Informaci√≥n valiosa:** Las descripciones S√ç contienen informaci√≥n relevante

### An√°lisis de Overfitting (Clasificaci√≥n por Ratios):
- ‚úÖ **Excelente (‚â§1.05):** Red Neuronal NLP Est√°ndar (1.007)
- ‚úÖ **Bueno (‚â§1.15):** XGBoost Optimizado (1.076), Red Neuronal Optimizada (0.992)
- ‚ö†Ô∏è **Moderado (‚â§1.50):** XGBoost NLP (1.487)
- ‚ùå **Severo (>1.50):** Random Forest NLP (1.686)

## üéØ Aplicaciones Pr√°cticas

### Para el Negocio:
- üè† **Sistema de valuaci√≥n autom√°tica** (error t√≠pico $26,471)
- üìä **Detecci√≥n de precios an√≥malos** (R¬≤ 0.898)
- üìà **Dashboard de monitoreo** por barrio/tipo de propiedad
- ü§ñ **Recomendaci√≥n de precios** basada en descripci√≥n textual
- üí¨ **An√°lisis de texto:** Identificaci√≥n de t√©rminos que incrementan valor

### Para Inversores:
- üí∞ **Identificaci√≥n de oportunidades** (modelo vs mercado)
- üìä **An√°lisis de mercado** por zona geogr√°fica (l2, l3)
- üîç **Evaluaci√≥n de propiedades** con descripciones optimizadas
- üìà **Predicci√≥n de ROI** basada en caracter√≠sticas textuales

### Para Desarrolladores:
- üèóÔ∏è **Optimizaci√≥n de descripciones** para maximizar valor percibido
- üìù **Guidelines de marketing** basadas en t√©rminos m√°s influyentes
- üéØ **Segmentaci√≥n de mercado** por preferencias textuales

## üî¨ Metodolog√≠a T√©cnica

### An√°lisis de Overfitting (Ratios train/test RMSE):
- **Excelente (‚â§1.05):** Generalizaci√≥n perfecta
- **Bueno (‚â§1.15):** Generalizaci√≥n aceptable  
- **Moderado (‚â§1.50):** Overfitting controlado
- **Severo (>1.50):** Overfitting problem√°tico

### Procesamiento NLP:
- **Limpieza:** Texto espa√±ol, stopwords personalizadas + inmobiliarias
- **Vectorizaci√≥n:** TF-IDF con unigramas y bigramas (max_features=5000)
- **Reducci√≥n:** SVD 5000‚Üí300 dimensiones (retiene >80% varianza)
- **Combinaci√≥n:** Features tradicionales (93) + texto (300) = 393 totales
- **Memoria:** Optimizaci√≥n para datasets grandes con batch processing

### Optimizaci√≥n de Hiperpar√°metros:
- **Random Forest:** HalvingRandomSearchCV con resource=n_estimators
- **XGBoost:** HalvingRandomSearchCV + early stopping
- **Redes Neuronales:** PyTorch con GPU, 3 arquitecturas diferentes
- **Validaci√≥n:** 2-fold CV para eficiencia en datasets grandes

## üîú Trabajo Futuro

1. **Embeddings avanzados:** Word2Vec, FastText, BERT en espa√±ol
2. **An√°lisis de sentimientos:** Polaridad y emociones en descripciones
3. **Geolocalizaci√≥n:** Features basadas en coordenadas lat/lon
4. **Ensemble methods:** Stacking de mejores modelos (XGB NLP + NN Simple)
5. **Optimizaci√≥n de memoria:** Streaming para datasets >1M registros
6. **Transfer learning:** Modelos pre-entrenados en espa√±ol inmobiliario
7. **Feature engineering:** Interacciones texto-num√©ricas autom√°ticas
8. **Deployment:** API REST para predicciones en tiempo real

## üèÜ Recomendaci√≥n Final

### **Modelo Recomendado para Producci√≥n:**
**XGBoost NLP** ($26,471 RMSE, R¬≤ 0.8982, MAE $17,284)

**Justificaci√≥n:**
- ‚úÖ **Mejor rendimiento:** 38% mejor que modelos tradicionales
- ‚ö†Ô∏è **Overfitting moderado:** 1.487 ratio (aceptable para el rendimiento obtenido)
- ‚úÖ **Escalabilidad:** Maneja bien 393 features h√≠bridas
- ‚úÖ **Interpretabilidad:** Feature importance clara y explicable
- ‚úÖ **Aplicabilidad:** Funciona con y sin descripci√≥n de texto
- ‚úÖ **ROI:** Reduce error de predicci√≥n significativamente vs alternativas

### **Modelo Alternativo Robusto:**
**Red Neuronal NLP Simple** ($28,193 RMSE, R¬≤ 0.8845)
- ‚úÖ **Excelente generalizaci√≥n:** Sin overfitting
- ‚úÖ **Performance s√≥lida:** 2¬∞ mejor modelo
- ‚úÖ **Estabilidad:** Ideal para ambientes conservadores

### **Estrategia H√≠brida Recomendada:**
1. **Producci√≥n principal:** XGBoost NLP (m√°ximo rendimiento)
2. **Validaci√≥n cruzada:** Red Neuronal Simple (robustez)
3. **Fallback:** XGBoost tradicional (cuando falta descripci√≥n)

## üë• Informaci√≥n del Curso

- **Materia:** Inteligencia de Negocios
- **Instituci√≥n:** Maestr√≠a en Econom√≠a Aplicada - UBA
- **A√±o:** 2025
- **Formato entrega:** Jupyter Notebooks ejecutados

---